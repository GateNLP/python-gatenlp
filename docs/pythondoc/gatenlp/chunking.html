<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>gatenlp.chunking API documentation</title>
<meta name="description" content="Module for chunking-related methods and annotators." />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>gatenlp.chunking</code></h1>
</header>
<section id="section-intro">
<p>Module for chunking-related methods and annotators.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
Module for chunking-related methods and annotators.
&#34;&#34;&#34;
import re
from typing import Union, List, Optional, Dict, Generator, Tuple
import iobes
from gatenlp import Document, Span, AnnotationSet

SPANENCS = dict(
    BIO=iobes.SpanEncoding.BIO,
    IOB=iobes.SpanEncoding.IOB,
    IOBES=iobes.SpanEncoding.IOBES,
    BILOU=iobes.SpanEncoding.BILOU,
    BMEOW=iobes.SpanEncoding.BMEOW,
    BMEWO=iobes.SpanEncoding.BMEWO,
)

PAT_WS = re.compile(r&#34;\s+&#34;)


def normalize_type(typename):
    &#34;&#34;&#34;
    Normalize the type name so it can be used as part of an BIO-like code that is usable in a conll-like dataset:
    replace all whitespace with a hyphen.

    Args:
        typename: the chunk type name

    Returns:
        normalized chunk type name
    &#34;&#34;&#34;
    return re.sub(PAT_WS, &#34;-&#34;, typename)


def doc_to_ibo(
        doc: Document,
        annset_name: str = &#34;&#34;,
        sentence_type: Optional[str] = None,
        token_type: str = &#34;Token&#34;,
        token_feature: Optional[str] = None,
        chunk_annset_name: Optional[str] = None,
        chunk_types: Optional[List[str]] = None,
        type2code: Optional[Dict] = None,
        scheme: str = &#34;BIO&#34;,
        return_rows:bool = True,
) -&gt; Generator[Union[List, Tuple], None, None]:
    &#34;&#34;&#34;
    Extract tokens and corresponding token entity codes.

    Args:
        doc: The document to process
        annset_name: name of the annotation set which contains all the types needed
        sentence_type: if None, use whole document, otherwise generate one result per sentence type annotation,
            if the sentence contains at least one token.
        token_type: type of token annotations to use
        token_feature: if not None, use the feature instead of the covered document text
        chunk_annset_name: is specified, the annotation set name to use for retrieving the chunk annotations,
            otherwise annset_name is used for the chunk annotations too.
        chunk_types: a list of annotation types which identify chunks, each chunk type is used as entity type
            Note the chunk type annotations must not overlap, but this is currently not checked, for performance
            reasons.
        type2code: an optionam dict mapping the chunk_type to the type name used in the BIO codes
        scheme: the encoding scheme to use, default is BIO, possible: IOB, BIO, IOBES, BILOU, BMEOW, BMEWO
        return_rows: if True, return a list of (tokenstring, code) tuples for each sentence, if False return two lists
            of equal length, the first with the token strings and the second with the codes

    Yields:
        either one list of (tokenstring, code) tuples per sentence found or two lists, one with the tokenstrings and
        the other with the codes.
    &#34;&#34;&#34;
    spanenc = SPANENCS[scheme]
    if type2code is None:
        type2code = {}
    if sentence_type is None:
        spans = [Span(0, len(doc))]
    else:
        spans = [a.span for a in doc.annset(annset_name).with_type(sentence_type)]
    all_tokens = doc.annset(annset_name).with_type(token_type)
    if chunk_types is None:
        all_chunks = AnnotationSet()
    else:
        all_chunks = doc.annset(annset_name if chunk_annset_name is None else chunk_annset_name).with_type(chunk_types)
    for span in spans:
        tokens = all_tokens.within(span)
        if len(tokens) == 0:
            continue
        # map token start offsets to token indices
        start2idx = {t.start: idx for idx, t in enumerate(tokens)}
        chunks = all_chunks.within(span)
        # now we want to know which of all the tokens are covered by chunks. So for each chunk, we check
        # which tokens are contained and append an iobes Span that points to the index of the token
        iobes_spans = []
        for chunk in chunks:
            ctokens = list(tokens.within(chunk))
            start = start2idx[ctokens[0].start]
            end = start2idx[ctokens[-1].start]+1
            iobes_span = iobes.Span(
                type=type2code.get(chunk.type, normalize_type(chunk.type)),
                start=start,
                end=end,
                tokens=tuple(range(start, end))
            )
            iobes_spans.append(iobes_span)
        codes = iobes.write_tags(iobes_spans, spanenc, length=len(tokens))
        assert len(tokens) == len(codes)
        if token_feature:
            token_strings = [t.features[token_feature] for t in tokens]
        else:
            token_strings = [doc[t] for t in tokens]
        if return_rows:
            yield [(t, c) for t, c in zip(token_strings, codes)]
        else:
            yield token_strings, codes</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="gatenlp.chunking.doc_to_ibo"><code class="name flex">
<span>def <span class="ident">doc_to_ibo</span></span>(<span>doc: <a title="gatenlp.document.Document" href="document.html#gatenlp.document.Document">Document</a>, annset_name: str = '', sentence_type: Optional[str] = None, token_type: str = 'Token', token_feature: Optional[str] = None, chunk_annset_name: Optional[str] = None, chunk_types: Optional[List[str]] = None, type2code: Optional[Dict[~KT, ~VT]] = None, scheme: str = 'BIO', return_rows: bool = True) ‑> Generator[Union[List[~T], Tuple[]], None, None]</span>
</code></dt>
<dd>
<div class="desc"><p>Extract tokens and corresponding token entity codes.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>doc</code></strong></dt>
<dd>The document to process</dd>
<dt><strong><code>annset_name</code></strong></dt>
<dd>name of the annotation set which contains all the types needed</dd>
<dt><strong><code>sentence_type</code></strong></dt>
<dd>if None, use whole document, otherwise generate one result per sentence type annotation,
if the sentence contains at least one token.</dd>
<dt><strong><code>token_type</code></strong></dt>
<dd>type of token annotations to use</dd>
<dt><strong><code>token_feature</code></strong></dt>
<dd>if not None, use the feature instead of the covered document text</dd>
<dt><strong><code>chunk_annset_name</code></strong></dt>
<dd>is specified, the annotation set name to use for retrieving the chunk annotations,
otherwise annset_name is used for the chunk annotations too.</dd>
<dt><strong><code>chunk_types</code></strong></dt>
<dd>a list of annotation types which identify chunks, each chunk type is used as entity type
Note the chunk type annotations must not overlap, but this is currently not checked, for performance
reasons.</dd>
<dt><strong><code>type2code</code></strong></dt>
<dd>an optionam dict mapping the chunk_type to the type name used in the BIO codes</dd>
<dt><strong><code>scheme</code></strong></dt>
<dd>the encoding scheme to use, default is BIO, possible: IOB, BIO, IOBES, BILOU, BMEOW, BMEWO</dd>
<dt><strong><code>return_rows</code></strong></dt>
<dd>if True, return a list of (tokenstring, code) tuples for each sentence, if False return two lists
of equal length, the first with the token strings and the second with the codes</dd>
</dl>
<h2 id="yields">Yields</h2>
<p>either one list of (tokenstring, code) tuples per sentence found or two lists, one with the tokenstrings and
the other with the codes.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def doc_to_ibo(
        doc: Document,
        annset_name: str = &#34;&#34;,
        sentence_type: Optional[str] = None,
        token_type: str = &#34;Token&#34;,
        token_feature: Optional[str] = None,
        chunk_annset_name: Optional[str] = None,
        chunk_types: Optional[List[str]] = None,
        type2code: Optional[Dict] = None,
        scheme: str = &#34;BIO&#34;,
        return_rows:bool = True,
) -&gt; Generator[Union[List, Tuple], None, None]:
    &#34;&#34;&#34;
    Extract tokens and corresponding token entity codes.

    Args:
        doc: The document to process
        annset_name: name of the annotation set which contains all the types needed
        sentence_type: if None, use whole document, otherwise generate one result per sentence type annotation,
            if the sentence contains at least one token.
        token_type: type of token annotations to use
        token_feature: if not None, use the feature instead of the covered document text
        chunk_annset_name: is specified, the annotation set name to use for retrieving the chunk annotations,
            otherwise annset_name is used for the chunk annotations too.
        chunk_types: a list of annotation types which identify chunks, each chunk type is used as entity type
            Note the chunk type annotations must not overlap, but this is currently not checked, for performance
            reasons.
        type2code: an optionam dict mapping the chunk_type to the type name used in the BIO codes
        scheme: the encoding scheme to use, default is BIO, possible: IOB, BIO, IOBES, BILOU, BMEOW, BMEWO
        return_rows: if True, return a list of (tokenstring, code) tuples for each sentence, if False return two lists
            of equal length, the first with the token strings and the second with the codes

    Yields:
        either one list of (tokenstring, code) tuples per sentence found or two lists, one with the tokenstrings and
        the other with the codes.
    &#34;&#34;&#34;
    spanenc = SPANENCS[scheme]
    if type2code is None:
        type2code = {}
    if sentence_type is None:
        spans = [Span(0, len(doc))]
    else:
        spans = [a.span for a in doc.annset(annset_name).with_type(sentence_type)]
    all_tokens = doc.annset(annset_name).with_type(token_type)
    if chunk_types is None:
        all_chunks = AnnotationSet()
    else:
        all_chunks = doc.annset(annset_name if chunk_annset_name is None else chunk_annset_name).with_type(chunk_types)
    for span in spans:
        tokens = all_tokens.within(span)
        if len(tokens) == 0:
            continue
        # map token start offsets to token indices
        start2idx = {t.start: idx for idx, t in enumerate(tokens)}
        chunks = all_chunks.within(span)
        # now we want to know which of all the tokens are covered by chunks. So for each chunk, we check
        # which tokens are contained and append an iobes Span that points to the index of the token
        iobes_spans = []
        for chunk in chunks:
            ctokens = list(tokens.within(chunk))
            start = start2idx[ctokens[0].start]
            end = start2idx[ctokens[-1].start]+1
            iobes_span = iobes.Span(
                type=type2code.get(chunk.type, normalize_type(chunk.type)),
                start=start,
                end=end,
                tokens=tuple(range(start, end))
            )
            iobes_spans.append(iobes_span)
        codes = iobes.write_tags(iobes_spans, spanenc, length=len(tokens))
        assert len(tokens) == len(codes)
        if token_feature:
            token_strings = [t.features[token_feature] for t in tokens]
        else:
            token_strings = [doc[t] for t in tokens]
        if return_rows:
            yield [(t, c) for t, c in zip(token_strings, codes)]
        else:
            yield token_strings, codes</code></pre>
</details>
</dd>
<dt id="gatenlp.chunking.normalize_type"><code class="name flex">
<span>def <span class="ident">normalize_type</span></span>(<span>typename)</span>
</code></dt>
<dd>
<div class="desc"><p>Normalize the type name so it can be used as part of an BIO-like code that is usable in a conll-like dataset:
replace all whitespace with a hyphen.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>typename</code></strong></dt>
<dd>the chunk type name</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>normalized chunk type name</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def normalize_type(typename):
    &#34;&#34;&#34;
    Normalize the type name so it can be used as part of an BIO-like code that is usable in a conll-like dataset:
    replace all whitespace with a hyphen.

    Args:
        typename: the chunk type name

    Returns:
        normalized chunk type name
    &#34;&#34;&#34;
    return re.sub(PAT_WS, &#34;-&#34;, typename)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="gatenlp" href="index.html">gatenlp</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="gatenlp.chunking.doc_to_ibo" href="#gatenlp.chunking.doc_to_ibo">doc_to_ibo</a></code></li>
<li><code><a title="gatenlp.chunking.normalize_type" href="#gatenlp.chunking.normalize_type">normalize_type</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>