<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>gatenlp.processing.tokenizer API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>gatenlp.processing.tokenizer</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import inspect
import types
import regex
from gatenlp.processing.annotator import Annotator

# NOTE we use NLTK&#39;s own aligner, but there is also get_original_spans(tk, s) from package tokenizations
from nltk.tokenize.util import align_tokens

# Generate token and optionally space token annotations in the given output annotation set.
# Optionally give non-default type names to the annotations.
# Some tokenizers may also add additional overlapping annotations (e.g. URL, EMAIL) for some tokens
# Tokenizers may have means like lookup tables and patterns for language specific tokenization.
# Tokenizers may have default patterns which work for several languages but should get initialized
# with the language-specific resources in the per-language package.

# in here we could also include pos taggers and lemmatizers as these are related to token features?


class Tokenizer(Annotator):
    &#34;&#34;&#34;
    A tokenizer creates token annotations and optionally also space token annotations. In additiona it
    may add word annotations for multi-word tokens and and multi-token words.

    Tokenizers should have the fields token_type, space_token_type, and word_type which identify
    the types of annotations it creates, and outset_name to identify the output annotation set.

    &#34;&#34;&#34;

    pass


class NLTKTokenizer(Tokenizer):
    &#34;&#34;&#34;
    Uses a NLTK Tokenizer to perform tokenization.
    &#34;&#34;&#34;

    def __init__(
        self, nltk_tokenizer=None, nltk_sent_tokenizer=None, outset_name=&#34;&#34;, token_type=&#34;Token&#34;, space_token_type=None
    ):
        &#34;&#34;&#34;
        Creates the tokenizer. NOTE: this tokenizer does NOT create space tokens by default

        Args:
            nltk_tokenizer: either a class or instance of an nltk tokenizer, or a tokenizer function
                that returns a list of tokens. NOTE: this must be a NLTK tokenizer which supports the
                span_tokenize method or a tokenizer or function that is not destructive, i.e. it must
                be possible to align the created token strings back to the original text. The created
                token strings must not be modified from the original text e.g. &#34; converted to `` or
                similar. For this reason the standard NLTKWordTokenizer cannot be used.
            nltk_sent_tokenizer: some NLTK tokenizers only work correctly if applied to each sentence
                separately after the text is splitted into sentences. This allows to specify an NLTK
                sentence splitter or a sentence splitting function to be run before the word tokenizer.
                Note: if the sentence tokenizer is used the `span_tokenize` method of a tokenizer will
                not be used if it exists, the `tokenize` method will be used.
            outset_name: annotation set to put the Token annotations in
            token_type: annotation type of the Token annotations
        &#34;&#34;&#34;
        assert nltk_tokenizer is not None
        if inspect.isclass(nltk_tokenizer):
            nltk_tokenizer = nltk_tokenizer()
        self.tokenizer = nltk_tokenizer
        # good idea but the method actually exists so instead we call it and if we get
        # an exception (which is a NotImplementedError) we set this to false
        # self.has_span_tokenize = hasattr(nltk_tokenizer, &#34;span_tokenize&#34;) and \
        #                         callable(getattr(nltk_tokenizer, &#34;span_tokenize&#34;))
        self.has_span_tokenize = True
        self.is_function = False
        if isinstance(self.tokenizer, types.FunctionType):
            self.has_span_tokenize = False
            self.is_function = True
        else:
            if nltk_sent_tokenizer is not None:
                self.has_span_tokenize = False
            else:
                try:
                    self.tokenizer.span_tokenize(&#34;text&#34;)
                except Exception as ex:
                    self.has_span_tokenize = False
        self.sent_tokenizer = nltk_sent_tokenizer
        self.sent_is_function = False
        if self.sent_tokenizer and isinstance(self.sent_tokenizer, types.FunctionType):
            self.sent_is_function = True
        self.outset_name = outset_name
        self.token_type = token_type
        self.space_token_type = space_token_type

    def __call__(self, doc, **kwargs):
        if doc.text is None:
            return doc
        if self.has_span_tokenize:
            # this may return a generator, convert to list so we can reuse
            spans = list(self.tokenizer.span_tokenize(doc.text))
        else:
            if self.sent_tokenizer:
                if self.sent_is_function:
                    sents = self.sent_tokenizer(doc.text)
                else:
                    sents = self.sent_tokenizer.tokenize(doc.text)
            else:
                sents = [doc.text]
            print(f&#34;DEBUG: sentences= {sents}&#34;)
            if self.is_function:
                tks = [self.tokenizer(s) for s in sents]
            else:
                tks = [self.tokenizer.tokenize(s) for s in sents]
            flat_tks = []
            for tk in tks:
                flat_tks.extend(tk)
            spans = align_tokens(flat_tks, doc.text)
        annset = doc.annset(self.outset_name)
        for span in spans:
            annset.add(span[0], span[1], self.token_type)
        if self.space_token_type is not None:
            last_off = 0
            for span in spans:
                if span[0] &gt; last_off:
                    annset.add(last_off, span[0], self.space_token_type)
                    last_off = span[1]
                else:
                    last_off = span[1]
            if last_off &lt; len(doc.text):
                annset.add(last_off, len(doc.text), self.space_token_type)
        return doc


class SplitPatternTokenizer(Tokenizer):
    &#34;&#34;&#34;
    Create annotations for spans of text defined by some literal or regular expression split pattern
    between those spans. Optionally also create annotations for the spans that match the split pattern.
    &#34;&#34;&#34;
    # TODO: how to properly use type hinting for regex/re patterns?
    def __init__(self,
                 split_pattern: any = regex.compile(r&#34;\s+&#34;),
                 token_pattern: any = None,
                 outset_name: str = &#34;&#34;,
                 token_type: str = &#34;Token&#34;,
                 space_token_type: str = None):
        &#34;&#34;&#34;
        Initialize the SplitPatternTokenizer.
        The pattern is either a literal string or a compiled regular expression.

        Args:
            split_pattern: a literal string or a compiled regular expression to find spans which split the text into
                tokens (default: any sequence of one or more whitespace characters)
            token_pattern: if not None, a token annotation is only created if the span between splits (or the begin
                or end of document and a split) matches this pattern: if a literal string, the literal string must
                be present, otherwise must be a compiled regular expression that is found.
            outset_name: the destination annotation set
            token_type: the type of annotation to create for the spans between splits
            space_token_type: if not None, the type of annotation to create for the splits. NOTE: non-splits which
                do not match the token_pattern are not annotated by this!
        &#34;&#34;&#34;
        self.split_pattern = split_pattern
        self.token_pattern = token_pattern
        self.outset_name = outset_name
        self.token_type = token_type
        self.space_token_type = space_token_type

    def _match_token_pattern(self, text):
        if isinstance(self.token_pattern, str):
            return text.find(self.token_pattern) &gt;= 0
        else:
            return self.token_pattern.search(text)

    def __call__(self, doc, **kwargs):
        annset = doc.annset(self.outset_name)
        last_off = 0
        if isinstance(self.split_pattern, str):
            l = len(self.split_pattern)
            idx = doc.text.find(self.split_pattern)
            while idx &gt; -1:
                if self.space_token_type is not None:
                    annset.add(idx, idx+l, self.space_token_type)
                if idx &gt; last_off:
                    if self.token_pattern is None or (
                            self.token_pattern and self._match_token_pattern(doc.text[last_off:idx])):
                        annset.add(last_off, idx, self.token_type)
                last_off = idx+len(self.split_pattern)
                idx = doc.text.find(self.split_pattern, idx+1)
        else:
            for m in self.split_pattern.finditer(doc.text):
                if self.space_token_type is not None:
                    annset.add(m.start(), m.end(), self.space_token_type)
                if m.start() &gt; last_off:
                    if self.token_pattern is None or (
                            self.token_pattern and self._match_token_pattern(doc.text[last_off:m.start()])):
                        annset.add(last_off, m.start(), self.token_type)
                last_off = m.end()
        if last_off &lt; len(doc.text):
            if self.token_pattern is None or (
                    self.token_pattern and self._match_token_pattern(doc.text[last_off, len(doc.text)])):
                annset.add(last_off, len(doc.text), self.token_type)
        return doc


class ParagraphTokenizer(SplitPatternTokenizer):
    &#34;&#34;&#34;
    Splits a document into paragraphs, based on the presence of one or more or two or more new lines.
    This is a convenience subclass of SplitPatternTokenizer, for more complex ways to split into paragraphs,
    that class should get used directly.
    &#34;&#34;&#34;
    def __init__(self, n_nl=1, outset_name=&#34;&#34;, paragraph_type=&#34;Paragraph&#34;, split_type=None):
        import re
        nl_str = &#34;\\n&#34; * n_nl
        pat = re.compile(nl_str+&#34;\\n*&#34;)
        super().__init__(split_pattern=pat, token_type=paragraph_type, space_token_type=split_type, outset_name=outset_name)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="gatenlp.processing.tokenizer.NLTKTokenizer"><code class="flex name class">
<span>class <span class="ident">NLTKTokenizer</span></span>
<span>(</span><span>nltk_tokenizer=None, nltk_sent_tokenizer=None, outset_name='', token_type='Token', space_token_type=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Uses a NLTK Tokenizer to perform tokenization.</p>
<p>Creates the tokenizer. NOTE: this tokenizer does NOT create space tokens by default</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>nltk_tokenizer</code></strong></dt>
<dd>either a class or instance of an nltk tokenizer, or a tokenizer function
that returns a list of tokens. NOTE: this must be a NLTK tokenizer which supports the
span_tokenize method or a tokenizer or function that is not destructive, i.e. it must
be possible to align the created token strings back to the original text. The created
token strings must not be modified from the original text e.g. " converted to `` or
similar. For this reason the standard NLTKWordTokenizer cannot be used.</dd>
<dt><strong><code>nltk_sent_tokenizer</code></strong></dt>
<dd>some NLTK tokenizers only work correctly if applied to each sentence
separately after the text is splitted into sentences. This allows to specify an NLTK
sentence splitter or a sentence splitting function to be run before the word tokenizer.
Note: if the sentence tokenizer is used the <code>span_tokenize</code> method of a tokenizer will
not be used if it exists, the <code>tokenize</code> method will be used.</dd>
<dt><strong><code>outset_name</code></strong></dt>
<dd>annotation set to put the Token annotations in</dd>
<dt><strong><code>token_type</code></strong></dt>
<dd>annotation type of the Token annotations</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class NLTKTokenizer(Tokenizer):
    &#34;&#34;&#34;
    Uses a NLTK Tokenizer to perform tokenization.
    &#34;&#34;&#34;

    def __init__(
        self, nltk_tokenizer=None, nltk_sent_tokenizer=None, outset_name=&#34;&#34;, token_type=&#34;Token&#34;, space_token_type=None
    ):
        &#34;&#34;&#34;
        Creates the tokenizer. NOTE: this tokenizer does NOT create space tokens by default

        Args:
            nltk_tokenizer: either a class or instance of an nltk tokenizer, or a tokenizer function
                that returns a list of tokens. NOTE: this must be a NLTK tokenizer which supports the
                span_tokenize method or a tokenizer or function that is not destructive, i.e. it must
                be possible to align the created token strings back to the original text. The created
                token strings must not be modified from the original text e.g. &#34; converted to `` or
                similar. For this reason the standard NLTKWordTokenizer cannot be used.
            nltk_sent_tokenizer: some NLTK tokenizers only work correctly if applied to each sentence
                separately after the text is splitted into sentences. This allows to specify an NLTK
                sentence splitter or a sentence splitting function to be run before the word tokenizer.
                Note: if the sentence tokenizer is used the `span_tokenize` method of a tokenizer will
                not be used if it exists, the `tokenize` method will be used.
            outset_name: annotation set to put the Token annotations in
            token_type: annotation type of the Token annotations
        &#34;&#34;&#34;
        assert nltk_tokenizer is not None
        if inspect.isclass(nltk_tokenizer):
            nltk_tokenizer = nltk_tokenizer()
        self.tokenizer = nltk_tokenizer
        # good idea but the method actually exists so instead we call it and if we get
        # an exception (which is a NotImplementedError) we set this to false
        # self.has_span_tokenize = hasattr(nltk_tokenizer, &#34;span_tokenize&#34;) and \
        #                         callable(getattr(nltk_tokenizer, &#34;span_tokenize&#34;))
        self.has_span_tokenize = True
        self.is_function = False
        if isinstance(self.tokenizer, types.FunctionType):
            self.has_span_tokenize = False
            self.is_function = True
        else:
            if nltk_sent_tokenizer is not None:
                self.has_span_tokenize = False
            else:
                try:
                    self.tokenizer.span_tokenize(&#34;text&#34;)
                except Exception as ex:
                    self.has_span_tokenize = False
        self.sent_tokenizer = nltk_sent_tokenizer
        self.sent_is_function = False
        if self.sent_tokenizer and isinstance(self.sent_tokenizer, types.FunctionType):
            self.sent_is_function = True
        self.outset_name = outset_name
        self.token_type = token_type
        self.space_token_type = space_token_type

    def __call__(self, doc, **kwargs):
        if doc.text is None:
            return doc
        if self.has_span_tokenize:
            # this may return a generator, convert to list so we can reuse
            spans = list(self.tokenizer.span_tokenize(doc.text))
        else:
            if self.sent_tokenizer:
                if self.sent_is_function:
                    sents = self.sent_tokenizer(doc.text)
                else:
                    sents = self.sent_tokenizer.tokenize(doc.text)
            else:
                sents = [doc.text]
            print(f&#34;DEBUG: sentences= {sents}&#34;)
            if self.is_function:
                tks = [self.tokenizer(s) for s in sents]
            else:
                tks = [self.tokenizer.tokenize(s) for s in sents]
            flat_tks = []
            for tk in tks:
                flat_tks.extend(tk)
            spans = align_tokens(flat_tks, doc.text)
        annset = doc.annset(self.outset_name)
        for span in spans:
            annset.add(span[0], span[1], self.token_type)
        if self.space_token_type is not None:
            last_off = 0
            for span in spans:
                if span[0] &gt; last_off:
                    annset.add(last_off, span[0], self.space_token_type)
                    last_off = span[1]
                else:
                    last_off = span[1]
            if last_off &lt; len(doc.text):
                annset.add(last_off, len(doc.text), self.space_token_type)
        return doc</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="gatenlp.processing.tokenizer.Tokenizer" href="#gatenlp.processing.tokenizer.Tokenizer">Tokenizer</a></li>
<li><a title="gatenlp.processing.annotator.Annotator" href="annotator.html#gatenlp.processing.annotator.Annotator">Annotator</a></li>
<li>abc.ABC</li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="gatenlp.processing.tokenizer.Tokenizer" href="#gatenlp.processing.tokenizer.Tokenizer">Tokenizer</a></b></code>:
<ul class="hlist">
<li><code><a title="gatenlp.processing.tokenizer.Tokenizer.__call__" href="annotator.html#gatenlp.processing.annotator.Annotator.__call__">__call__</a></code></li>
<li><code><a title="gatenlp.processing.tokenizer.Tokenizer.finish" href="annotator.html#gatenlp.processing.annotator.Annotator.finish">finish</a></code></li>
<li><code><a title="gatenlp.processing.tokenizer.Tokenizer.pipe" href="annotator.html#gatenlp.processing.annotator.Annotator.pipe">pipe</a></code></li>
<li><code><a title="gatenlp.processing.tokenizer.Tokenizer.reduce" href="annotator.html#gatenlp.processing.annotator.Annotator.reduce">reduce</a></code></li>
<li><code><a title="gatenlp.processing.tokenizer.Tokenizer.start" href="annotator.html#gatenlp.processing.annotator.Annotator.start">start</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="gatenlp.processing.tokenizer.ParagraphTokenizer"><code class="flex name class">
<span>class <span class="ident">ParagraphTokenizer</span></span>
<span>(</span><span>n_nl=1, outset_name='', paragraph_type='Paragraph', split_type=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Splits a document into paragraphs, based on the presence of one or more or two or more new lines.
This is a convenience subclass of SplitPatternTokenizer, for more complex ways to split into paragraphs,
that class should get used directly.</p>
<p>Initialize the SplitPatternTokenizer.
The pattern is either a literal string or a compiled regular expression.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>split_pattern</code></strong></dt>
<dd>a literal string or a compiled regular expression to find spans which split the text into
tokens (default: any sequence of one or more whitespace characters)</dd>
<dt><strong><code>token_pattern</code></strong></dt>
<dd>if not None, a token annotation is only created if the span between splits (or the begin
or end of document and a split) matches this pattern: if a literal string, the literal string must
be present, otherwise must be a compiled regular expression that is found.</dd>
<dt><strong><code>outset_name</code></strong></dt>
<dd>the destination annotation set</dd>
<dt><strong><code>token_type</code></strong></dt>
<dd>the type of annotation to create for the spans between splits</dd>
<dt><strong><code>space_token_type</code></strong></dt>
<dd>if not None, the type of annotation to create for the splits. NOTE: non-splits which
do not match the token_pattern are not annotated by this!</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ParagraphTokenizer(SplitPatternTokenizer):
    &#34;&#34;&#34;
    Splits a document into paragraphs, based on the presence of one or more or two or more new lines.
    This is a convenience subclass of SplitPatternTokenizer, for more complex ways to split into paragraphs,
    that class should get used directly.
    &#34;&#34;&#34;
    def __init__(self, n_nl=1, outset_name=&#34;&#34;, paragraph_type=&#34;Paragraph&#34;, split_type=None):
        import re
        nl_str = &#34;\\n&#34; * n_nl
        pat = re.compile(nl_str+&#34;\\n*&#34;)
        super().__init__(split_pattern=pat, token_type=paragraph_type, space_token_type=split_type, outset_name=outset_name)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="gatenlp.processing.tokenizer.SplitPatternTokenizer" href="#gatenlp.processing.tokenizer.SplitPatternTokenizer">SplitPatternTokenizer</a></li>
<li><a title="gatenlp.processing.tokenizer.Tokenizer" href="#gatenlp.processing.tokenizer.Tokenizer">Tokenizer</a></li>
<li><a title="gatenlp.processing.annotator.Annotator" href="annotator.html#gatenlp.processing.annotator.Annotator">Annotator</a></li>
<li>abc.ABC</li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="gatenlp.processing.tokenizer.SplitPatternTokenizer" href="#gatenlp.processing.tokenizer.SplitPatternTokenizer">SplitPatternTokenizer</a></b></code>:
<ul class="hlist">
<li><code><a title="gatenlp.processing.tokenizer.SplitPatternTokenizer.__call__" href="annotator.html#gatenlp.processing.annotator.Annotator.__call__">__call__</a></code></li>
<li><code><a title="gatenlp.processing.tokenizer.SplitPatternTokenizer.finish" href="annotator.html#gatenlp.processing.annotator.Annotator.finish">finish</a></code></li>
<li><code><a title="gatenlp.processing.tokenizer.SplitPatternTokenizer.pipe" href="annotator.html#gatenlp.processing.annotator.Annotator.pipe">pipe</a></code></li>
<li><code><a title="gatenlp.processing.tokenizer.SplitPatternTokenizer.reduce" href="annotator.html#gatenlp.processing.annotator.Annotator.reduce">reduce</a></code></li>
<li><code><a title="gatenlp.processing.tokenizer.SplitPatternTokenizer.start" href="annotator.html#gatenlp.processing.annotator.Annotator.start">start</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="gatenlp.processing.tokenizer.SplitPatternTokenizer"><code class="flex name class">
<span>class <span class="ident">SplitPatternTokenizer</span></span>
<span>(</span><span>split_pattern: <built-in function any> = regex.Regex('\\s+', flags=regex.V0), token_pattern: <built-in function any> = None, outset_name: str = '', token_type: str = 'Token', space_token_type: str = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Create annotations for spans of text defined by some literal or regular expression split pattern
between those spans. Optionally also create annotations for the spans that match the split pattern.</p>
<p>Initialize the SplitPatternTokenizer.
The pattern is either a literal string or a compiled regular expression.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>split_pattern</code></strong></dt>
<dd>a literal string or a compiled regular expression to find spans which split the text into
tokens (default: any sequence of one or more whitespace characters)</dd>
<dt><strong><code>token_pattern</code></strong></dt>
<dd>if not None, a token annotation is only created if the span between splits (or the begin
or end of document and a split) matches this pattern: if a literal string, the literal string must
be present, otherwise must be a compiled regular expression that is found.</dd>
<dt><strong><code>outset_name</code></strong></dt>
<dd>the destination annotation set</dd>
<dt><strong><code>token_type</code></strong></dt>
<dd>the type of annotation to create for the spans between splits</dd>
<dt><strong><code>space_token_type</code></strong></dt>
<dd>if not None, the type of annotation to create for the splits. NOTE: non-splits which
do not match the token_pattern are not annotated by this!</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SplitPatternTokenizer(Tokenizer):
    &#34;&#34;&#34;
    Create annotations for spans of text defined by some literal or regular expression split pattern
    between those spans. Optionally also create annotations for the spans that match the split pattern.
    &#34;&#34;&#34;
    # TODO: how to properly use type hinting for regex/re patterns?
    def __init__(self,
                 split_pattern: any = regex.compile(r&#34;\s+&#34;),
                 token_pattern: any = None,
                 outset_name: str = &#34;&#34;,
                 token_type: str = &#34;Token&#34;,
                 space_token_type: str = None):
        &#34;&#34;&#34;
        Initialize the SplitPatternTokenizer.
        The pattern is either a literal string or a compiled regular expression.

        Args:
            split_pattern: a literal string or a compiled regular expression to find spans which split the text into
                tokens (default: any sequence of one or more whitespace characters)
            token_pattern: if not None, a token annotation is only created if the span between splits (or the begin
                or end of document and a split) matches this pattern: if a literal string, the literal string must
                be present, otherwise must be a compiled regular expression that is found.
            outset_name: the destination annotation set
            token_type: the type of annotation to create for the spans between splits
            space_token_type: if not None, the type of annotation to create for the splits. NOTE: non-splits which
                do not match the token_pattern are not annotated by this!
        &#34;&#34;&#34;
        self.split_pattern = split_pattern
        self.token_pattern = token_pattern
        self.outset_name = outset_name
        self.token_type = token_type
        self.space_token_type = space_token_type

    def _match_token_pattern(self, text):
        if isinstance(self.token_pattern, str):
            return text.find(self.token_pattern) &gt;= 0
        else:
            return self.token_pattern.search(text)

    def __call__(self, doc, **kwargs):
        annset = doc.annset(self.outset_name)
        last_off = 0
        if isinstance(self.split_pattern, str):
            l = len(self.split_pattern)
            idx = doc.text.find(self.split_pattern)
            while idx &gt; -1:
                if self.space_token_type is not None:
                    annset.add(idx, idx+l, self.space_token_type)
                if idx &gt; last_off:
                    if self.token_pattern is None or (
                            self.token_pattern and self._match_token_pattern(doc.text[last_off:idx])):
                        annset.add(last_off, idx, self.token_type)
                last_off = idx+len(self.split_pattern)
                idx = doc.text.find(self.split_pattern, idx+1)
        else:
            for m in self.split_pattern.finditer(doc.text):
                if self.space_token_type is not None:
                    annset.add(m.start(), m.end(), self.space_token_type)
                if m.start() &gt; last_off:
                    if self.token_pattern is None or (
                            self.token_pattern and self._match_token_pattern(doc.text[last_off:m.start()])):
                        annset.add(last_off, m.start(), self.token_type)
                last_off = m.end()
        if last_off &lt; len(doc.text):
            if self.token_pattern is None or (
                    self.token_pattern and self._match_token_pattern(doc.text[last_off, len(doc.text)])):
                annset.add(last_off, len(doc.text), self.token_type)
        return doc</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="gatenlp.processing.tokenizer.Tokenizer" href="#gatenlp.processing.tokenizer.Tokenizer">Tokenizer</a></li>
<li><a title="gatenlp.processing.annotator.Annotator" href="annotator.html#gatenlp.processing.annotator.Annotator">Annotator</a></li>
<li>abc.ABC</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="gatenlp.processing.tokenizer.ParagraphTokenizer" href="#gatenlp.processing.tokenizer.ParagraphTokenizer">ParagraphTokenizer</a></li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="gatenlp.processing.tokenizer.Tokenizer" href="#gatenlp.processing.tokenizer.Tokenizer">Tokenizer</a></b></code>:
<ul class="hlist">
<li><code><a title="gatenlp.processing.tokenizer.Tokenizer.__call__" href="annotator.html#gatenlp.processing.annotator.Annotator.__call__">__call__</a></code></li>
<li><code><a title="gatenlp.processing.tokenizer.Tokenizer.finish" href="annotator.html#gatenlp.processing.annotator.Annotator.finish">finish</a></code></li>
<li><code><a title="gatenlp.processing.tokenizer.Tokenizer.pipe" href="annotator.html#gatenlp.processing.annotator.Annotator.pipe">pipe</a></code></li>
<li><code><a title="gatenlp.processing.tokenizer.Tokenizer.reduce" href="annotator.html#gatenlp.processing.annotator.Annotator.reduce">reduce</a></code></li>
<li><code><a title="gatenlp.processing.tokenizer.Tokenizer.start" href="annotator.html#gatenlp.processing.annotator.Annotator.start">start</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="gatenlp.processing.tokenizer.Tokenizer"><code class="flex name class">
<span>class <span class="ident">Tokenizer</span></span>
</code></dt>
<dd>
<div class="desc"><p>A tokenizer creates token annotations and optionally also space token annotations. In additiona it
may add word annotations for multi-word tokens and and multi-token words.</p>
<p>Tokenizers should have the fields token_type, space_token_type, and word_type which identify
the types of annotations it creates, and outset_name to identify the output annotation set.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Tokenizer(Annotator):
    &#34;&#34;&#34;
    A tokenizer creates token annotations and optionally also space token annotations. In additiona it
    may add word annotations for multi-word tokens and and multi-token words.

    Tokenizers should have the fields token_type, space_token_type, and word_type which identify
    the types of annotations it creates, and outset_name to identify the output annotation set.

    &#34;&#34;&#34;

    pass</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="gatenlp.processing.annotator.Annotator" href="annotator.html#gatenlp.processing.annotator.Annotator">Annotator</a></li>
<li>abc.ABC</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="gatenlp.processing.tokenizer.NLTKTokenizer" href="#gatenlp.processing.tokenizer.NLTKTokenizer">NLTKTokenizer</a></li>
<li><a title="gatenlp.processing.tokenizer.SplitPatternTokenizer" href="#gatenlp.processing.tokenizer.SplitPatternTokenizer">SplitPatternTokenizer</a></li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="gatenlp.processing.annotator.Annotator" href="annotator.html#gatenlp.processing.annotator.Annotator">Annotator</a></b></code>:
<ul class="hlist">
<li><code><a title="gatenlp.processing.annotator.Annotator.__call__" href="annotator.html#gatenlp.processing.annotator.Annotator.__call__">__call__</a></code></li>
<li><code><a title="gatenlp.processing.annotator.Annotator.finish" href="annotator.html#gatenlp.processing.annotator.Annotator.finish">finish</a></code></li>
<li><code><a title="gatenlp.processing.annotator.Annotator.pipe" href="annotator.html#gatenlp.processing.annotator.Annotator.pipe">pipe</a></code></li>
<li><code><a title="gatenlp.processing.annotator.Annotator.reduce" href="annotator.html#gatenlp.processing.annotator.Annotator.reduce">reduce</a></code></li>
<li><code><a title="gatenlp.processing.annotator.Annotator.start" href="annotator.html#gatenlp.processing.annotator.Annotator.start">start</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="gatenlp.processing" href="index.html">gatenlp.processing</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="gatenlp.processing.tokenizer.NLTKTokenizer" href="#gatenlp.processing.tokenizer.NLTKTokenizer">NLTKTokenizer</a></code></h4>
</li>
<li>
<h4><code><a title="gatenlp.processing.tokenizer.ParagraphTokenizer" href="#gatenlp.processing.tokenizer.ParagraphTokenizer">ParagraphTokenizer</a></code></h4>
</li>
<li>
<h4><code><a title="gatenlp.processing.tokenizer.SplitPatternTokenizer" href="#gatenlp.processing.tokenizer.SplitPatternTokenizer">SplitPatternTokenizer</a></code></h4>
</li>
<li>
<h4><code><a title="gatenlp.processing.tokenizer.Tokenizer" href="#gatenlp.processing.tokenizer.Tokenizer">Tokenizer</a></code></h4>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>