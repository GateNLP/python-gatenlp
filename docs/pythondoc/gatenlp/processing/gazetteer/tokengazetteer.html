<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>gatenlp.processing.gazetteer.tokengazetteer API documentation</title>
<meta name="description" content="This module provides Gazetteer classes which allow matching the text or the tokens of documents against
gazetteer lists, lists of interesting texts or …" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>gatenlp.processing.gazetteer.tokengazetteer</code></h1>
</header>
<section id="section-intro">
<p>This module provides Gazetteer classes which allow matching the text or the tokens of documents against
gazetteer lists, lists of interesting texts or token sequences and annotate the matches with features from the
gazetteer lists.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
This module provides Gazetteer classes which allow matching the text or the tokens of documents against
gazetteer lists, lists of interesting texts or token sequences and annotate the matches with features from the
gazetteer lists.
&#34;&#34;&#34;

import os
from typing import Union, Dict, Optional, Callable, List
from collections import defaultdict
from recordclass import structclass

from gatenlp.document import Document, Annotation
from gatenlp.utils import init_logger
from gatenlp.processing.annotator import Annotator
from gatenlp.processing.gazetteer.base import GazetteerBase

# TODO: better handling/support for separator annotations: this would add complexity but allow that a sequence
#   of annotations is only matched if there is a/several? separator annotation between each of those annotations.
#   Could also require this only if there is a separator in the gazetteer sequence (e.g. indicated by a None element)


# NOTE! this was origiannl a @dataclass(unsafe_hash=True, order=True)
# class TokenGazetteerMatch, with __slots__=(&#34;start&#34;, &#34;end&#34;, &#34;match&#34;, &#34;entrydata&#34;, &#34;matcherdata&#34;)
# and type declarations start: int, end: int, match: list, entrydata: object, matcherdata: object
# HOWEVER, dataclasses require Python 3.7 and have their own issues.
# Named tuples cannot be used because what we need has to be mutable.
# So for now we use the structclass approach from package recordclass which is very compact and rather fast.
# !! structclass by default does NOT support cyclic garbage collection which should be ok for us


TokenGazetteerMatch = structclass(
    &#34;TokenGazetteerMatch&#34;, (&#34;start&#34;, &#34;end&#34;, &#34;match&#34;, &#34;data&#34;, &#34;listidx&#34;)
)


class TokenGazetteerNode:
    &#34;&#34;&#34;
    Represent an entry in the hash map of entry first tokens.
    If is_match is True, that token is already a match and data contains the entry data.
    The continuations attribute contains None or a list of multi token matches that
    start with the first token and the entry data if we have a match (all tokens match).
    &#34;&#34;&#34;

    __slots__ = (&#34;is_match&#34;, &#34;data&#34;, &#34;nodes&#34;, &#34;listidx&#34;)

    def __init__(self, is_match=None, data=None, nodes=None, listidx=None):
        &#34;&#34;&#34;

        Args:
            is_match: this node is a match
            data: data associated with the match, can be a list of data items
            nodes:
        &#34;&#34;&#34;
        self.is_match = is_match
        self.data = data
        self.listidx = listidx
        self.nodes = nodes

    @staticmethod
    def dict_repr(nodes):
        if nodes is not None:
            return str([(t, n) for t, n in nodes.items()])

    def __repr__(self):
        nodes = TokenGazetteerNode.dict_repr(self.nodes)
        return f&#34;Node(is_match={self.is_match},data={self.data},listidx={self.listidx},nodes={nodes})&#34;


def tokentext_getter(token, doc=None, feature=None):
    if feature is not None:
        txt = token.features.get(feature)
    else:
        if doc is None:
            raise Exception(&#34;No feature given, need doc for gazetteer&#34;)
        txt = doc[token]
    return txt


# TODO: allow output annotation type to be set from the match or from the list!
class TokenGazetteer(GazetteerBase):
    def __init__(
        self,
        source: Union[List, str, None] = None,
        source_fmt: str = &#34;gate-def&#34;,
        source_sep=&#34;\t&#34;,
        source_encoding=&#34;UTF-8&#34;,
        # cache_source=None,   # TODO
        source_tokenizer: Union[None, Annotator, Callable] = None,
        longest_only: bool = False,
        skip_longest: bool = False,
        outset_name: str = &#34;&#34;,
        ann_type: str = &#34;Lookup&#34;,
        annset_name: str = &#34;&#34;,
        token_type: str = &#34;Token&#34;,
        feature=None,
        split_type: Optional[str] = None,
        within_type: Optional[str] = None,
        mapfunc: Optional[Callable] = None,
        ignorefunc: Optional[Callable] = None,
        getterfunc: Optional[Callable] = None,
        list_features: Optional[Dict] = None,
        list_type: Optional[str] = None,
    ):
        &#34;&#34;&#34;

        Args:
            source: where to load the gazetteer from. What is actually expected here depends on the fmt
              parameter. If none, nothing is loaded
            source_fmt: defines what is expected as the format and/or content of the source parameter. One of:
               *  &#34;gate-def&#34; (default): the path to a GATE-style &#34;def&#34; file.
                  See https://gate.ac.uk/userguide/chap:gazetteers
               * &#34;gazlist&#34;: a list of tuples or lists where the first element of the tuple/list
                  is a list of strings and the second element is a dictionary containing the features to assign.
                  All entries in the list belong to the first gazetteer list which has list features as
                  specified with the listfeatures parameter and a list type as specified with the listtype parameter.
            source_sep: the field separator to use for some source formats (default: tab character)
            source_encoding: the encoding to use for some source formats (default: UTF-8)
            source_tokenizer: if not None, an annotator, that creates annotations of type &#34;Token&#34; in the default
                annotation set. If this is None, then when loading string gazetteer entries, they are tokenized by
                splitting on whitespace (as defined by Python str.split())
            feature: the feature name to use to get the string for each token. If the corresponding feature
                in the token does not exist, is None or is the empty string, the Token is completely ignored.
                If the feature parameter is None, use the document string covered by the token.
            longest_only: if True, only returns the longest match at each matching position, otherwise returns all
                matches.
            skip_longest: skip forward over longest match (do not return contained/overlapping matches)
            annset_name: the set where the tokens to match should come from
            outset_name: the set where the new annotations are added
            ann_type: the annotation type of the annotations to create, unless a type is given for the gazetteer
                entry or for the gazetteer list.
            token_type: the annotation type of the token annotations
            split_type: the annotation type of any split annotations which will end any ongoing match
            within_type: only matches fully within annotations of this type will be made
            mapfunc: a callable that maps the original string extracted for each token to the actual string to use.
            ignorefunc: a callable which given the mapped token string decides if the token should be ignored
                (not added to the gazetteer list, not considered in the document when matching)
            getterfunc: a callable which, given a token annotation, retrieves the string. If there is mapfunc, the
                retrieved string is then still run through the mapfunc. The getterfunc must accept the token and
                an optional document as parameters.
            list_features: a dictionary of features common to the whole list loaded or None.
                If what gets loaded specifies
                its own list features, this is getting ignored.
            list_type: the output annotation type to use for the list, ignored if the input format specifies this
                on its own. If the input does not specify this on its own and this is not None, then it takes
                precedence over outtype for the data loaded from source.

        &#34;&#34;&#34;
        self.nodes = defaultdict(TokenGazetteerNode)
        self.mapfunc = mapfunc
        self.ignorefunc = ignorefunc
        self.feature = feature
        self.annset = annset_name
        self.tokentype = token_type
        self.splittype = split_type
        self.withintype = within_type
        self.outset = outset_name
        self.outtype = ann_type
        self.longest_only = longest_only
        self.skip = skip_longest
        if getterfunc:
            self.getterfunc = getterfunc
        else:
            self.getterfunc = tokentext_getter
        self.listfeatures = []
        self.listtypes = []
        self.logger = init_logger(__name__)
        # self.logger.setLevel(logging.DEBUG)
        self.size = 0
        if source is not None:
            self.append(source, source_fmt=source_fmt,
                        list_features=list_features, list_type=list_type, source_sep=source_sep,
                        source_encoding=source_encoding, source_tokenizer=source_tokenizer
                        )

    def append(
        self,
        source:  Union[None, str, List],
        source_fmt: str = &#34;gate-def&#34;,
        source_sep: str = &#34;\t&#34;,
        source_encoding: str = &#34;UTF-8&#34;,
        source_tokenizer: Union[None, Annotator, Callable] = None,
        source_splitter: Optional[Callable] = None,
        list_features: Optional[Dict] = None,
        list_type: Optional[str] = None,
    ):
        &#34;&#34;&#34;
        This method appends more entries to gazetteer.

        Args:
            source: where to load the gazetteer from. What is actually expected here depends on the fmt
              parameter.
            source_fmt: defines what is expected as the format and/or content of the source parameter. One of:
               *  &#34;gate-def&#34; (default): source must be a string, a pathlib Path or a parsed urllib url and
                  point to a GATE-style &#34;def&#34; file. See https://gate.ac.uk/userguide/chap:gazetteers
               * &#34;gazlist&#34;: a list of tuples or lists where the first element of the tuple/list
                  is a list of strings, the second element is a dictionary containing the features to assign and
                  the third element, if it exists, is the index of an element in the listfeatures array.
            source_sep: the field separator to use for some source formats (default: tab character)
            source_encoding: the encoding to use for some source formats (default: UTF-8)
            source_tokenizer: if not None, an annotator, that creates annotations of type &#34;Token&#34; in the default
                annotation set. If this is None, then when loading string gazetteer entries, they are tokenized by
                splitting on whitespace (as defined by Python str.split())
            source_splitter: if not None and source_tokenizer is None, a callable that takes a string and returns
                the tokenstrings to use
            list_features: a list of dictionaries containing the features to set for all matches witch have the
              list index set, this list gets appended to the existing listfeatures. If what gets appended specifies
              its own list features, this is ignored.
            list_type: the output annotation type to use for the list that gets appended. If what gets appended
               specifies its own list type or list types, this is ignored.
        &#34;&#34;&#34;
        if source_fmt == &#34;gazlist&#34;:
            if list_features is not None:
                self.listfeatures.append(list_features)
            else:
                self.listfeatures.append({})
            if list_type is not None:
                self.listtypes.append(list_type)
            else:
                self.listtypes.append(self.outtype)
            listidx = len(self.listfeatures) - 1
            for el in source:
                entry = el[0]
                data = el[1]
                self.add(entry, data, listidx=listidx)
        elif source_fmt == &#34;gate-def&#34;:
            if list_features is None:
                list_features = {}
            if list_type is None:
                list_type = self.outtype
            with open(source, &#34;rt&#34;, encoding=source_encoding) as infp:
                for line in infp:
                    line = line.rstrip(&#34;\n\r&#34;)
                    fields = line.split(&#34;:&#34;)
                    fields.extend([&#34;&#34;, &#34;&#34;, &#34;&#34;, &#34;&#34;])
                    listFile = fields[0]
                    majorType = fields[1]
                    minorType = fields[2]
                    languages = fields[3]
                    anntype = fields[4]
                    this_listfeatures = list_features.copy()
                    this_outtype = list_type
                    if majorType:
                        this_listfeatures[&#34;majorType&#34;] = majorType
                    if minorType:
                        this_listfeatures[&#34;minorType&#34;] = minorType
                    if languages:
                        this_listfeatures[&#34;lang&#34;] = languages
                    if anntype:
                        this_outtype = anntype
                    # read in the actual list
                    listfile = os.path.join(os.path.dirname(source), listFile)
                    self.logger.debug(f&#34;Reading list file {listfile}&#34;)
                    with open(listfile, &#34;rt&#34;, encoding=source_encoding) as inlistfile:
                        self.listtypes.append(this_outtype)
                        self.listfeatures.append(this_listfeatures)
                        linenr = 0
                        for listline in inlistfile:
                            linenr += 1
                            listline = listline.rstrip(&#34;\n\r&#34;)
                            fields = listline.split(source_sep)
                            entry = fields[0]
                            if source_tokenizer or source_splitter:
                                if source_tokenizer:
                                    tmpdoc = Document(entry)
                                    tmpdoc = source_tokenizer(tmpdoc)  # we MUST reassign here to allow return of a new doc!
                                    tokenanns = list(tmpdoc.annset().with_type(&#34;Token&#34;))
                                    if self.getterfunc:
                                        tokenstrings = [
                                            self.getterfunc(a, doc=tmpdoc)
                                            for a in tokenanns
                                        ]
                                    else:
                                        tokenstrings = [tmpdoc[a] for a in tokenanns]
                                else:
                                    tokenstrings = source_splitter(entry)
                                if self.mapfunc:
                                    tokenstrings = [
                                        self.mapfunc(s) for s in tokenstrings
                                    ]
                                if self.ignorefunc:
                                    tokenstrings = [
                                        s
                                        for s in tokenstrings
                                        if not self.ignorefunc(s)
                                    ]
                            else:
                                tokenstrings = entry.split()  # just split on whitespace
                            if len(tokenstrings) == 0:
                                self.logger.warning(
                                    f&#34;File {listfile}, skipping line {linenr}, no tokens left: {listline}&#34;
                                )
                                continue
                            if len(entry) &gt; 1:
                                feats = {}
                                for fspec in fields[1:]:
                                    fname, fval = fspec.split(&#34;=&#34;)
                                    feats[fname] = fval
                            else:
                                feats = None
                            listidx = len(self.listfeatures) - 1
                            self.add(tokenstrings, feats, listidx=listidx)
        else:
            raise Exception(f&#34;TokenGazetteer format {source_fmt} not known&#34;)

    def add(self, entry, data=None, listidx=None):
        &#34;&#34;&#34;
        Add a single gazetteer entry. A gazetteer entry can have no data associated with it at all if both
        data and listidx are None. If only list indices are given then an array of those indices is stored
        with the entry and data remaines None, if only data is given then an array of data is stored and
        listidx remains None. If at some point, both data and a listidx are stored in the same entry, then
        both fields are changed to have both a list with the same number of elements corresponding to each
        other, with missing data or listidx elements being None.

        Args:
            entry: a iterable of string or a string for a single element, each element is the string that
               represents a token to be matched
            data: dictionary of features to add
            listidx: the index to list features and a list type to add
        &#34;&#34;&#34;
        if isinstance(entry, str):
            entry = [entry]
        node = None
        i = 0
        for token in entry:
            if self.mapfunc is not None:
                token = self.mapfunc(token)
            if self.ignorefunc is not None and self.ignorefunc(token):
                continue
            if i == 0:
                node = self.nodes[token]
            else:
                if node.nodes is None:
                    node.nodes = defaultdict(TokenGazetteerNode)
                    tmpnode = TokenGazetteerNode()
                    node.nodes[token] = tmpnode
                    node = tmpnode
                else:
                    node = node.nodes[token]
            i += 1
        node.is_match = True
        self.size += 1
        # For now: always store parallel lists of data and listidxs, with None elements if necessary.
        if data is not None or listidx is not None:
            if node.data is None:
                node.data = [data]
                node.listidx = [listidx]
            else:
                node.data.append(data)
                node.listidx.append(listidx)

        # TODO: code to test and correct: try to save space by only storing parallel lists if
        # both data and listindices are actually both non-null and added:
        #
        # if data is None and listidx is None:
        #     # nothing to do, return what we have
        #     return node.data, node.listidx
        # # if we have only data and no listidx and there is no listidx
        # if data is not None and listidx is None and node.listidx is None:
        #     if node.data is None:
        #         node.data = [data]
        #     else:
        #         node.data.append(data)
        # elif listidx is not None and data is None and node.data is None:
        #     if node.listidx is None:
        #         node.listidx = [listidx]
        #     else:
        #         node.listidx.append(listidx)
        # else:
        #     # make sure we have parallel lists
        #     if node.data is None:
        #         node.data = []
        #     if node.listidx is None:
        #         node.listidx = []
        #     if len(node.data) &gt; len(node.listidx):
        #         node.listidx.extend([None] * (len(node.data) - len(node.listidx)))
        #     elif len(node.listidx) &gt; len(node.data):
        #         node.data.extend([None] * (len(node.listidx) - len(node.data)))
        #     if listidx:
        #         node.listidx.append(listidx)
        #         if data:
        #             node.data.append(data)
        #         else:
        #             node.data.append(None)
        #     else:
        #         node.listidx.append(None)
        #         node.listidx.append(listidx)

    def match(self, tokens, doc=None, longest_only=None, idx=0, endidx=None, matchfunc=None):
        &#34;&#34;&#34;
        Try to match at index location idx of the tokens sequence. Returns a list which contains
        no elements if no match is found,  or
        as many elements as matches are found. The element for each match is either a
        TokenGazeteerMatch instance if matchfunc is None or whatever matchfunc returns for a match.
        Also returns the legngth of the longest match (0 if no match).

        Args:
            tokens: a list of tokens (must allow to fetch the ith token as tokens[i])
            doc: the document to which the tokens belong. Necessary of the underlying text is used
               for the tokens.
            longest_only: whether to return all matches or just the longest ones. If None, overrides the setting
               from init.
            idx: the index in tokens where the match must start
            endidx: the index in tokens after which no match must end
            matchfunc: a function to process each match.
               The function is passed the TokenGazetteerMatch and the doc and should return something
               that is then added to the result list of matches.

        Returns:
            A tuple, where the first element is a list of match elements, empty if no matches are found
            and the second element is the length of the longest match, 0 if no match.

        &#34;&#34;&#34;
        if endidx is None:
            endidx = len(tokens)
        assert idx &lt; endidx
        if longest_only is None:
            longest_only = self.longest_only
        token = tokens[idx]
        if token.type == self.splittype:
            return [], 0
        token_string = self.getterfunc(token, doc=doc, feature=self.feature)
        if token_string is None:
            return [], 0
        if self.mapfunc:
            token_string = self.mapfunc(token_string)
        if self.ignorefunc:
            if self.ignorefunc(token_string):
                # no match possible here
                return [], 0
        # check if we can match the current token
        if token_string in self.nodes:
            # ok, we have the beginning of a possible match
            longest = 0
            node = self.nodes[token_string]
            thismatches = []
            thistokens = [token]
            if node.is_match:
                # the first token is already a complete match, so we need to add this to thismatches
                longest = 1
                # TODO: make this work with list data!
                if matchfunc:
                    match = matchfunc(
                        idx, idx + 1, thistokens.copy(), node.data, node.listidx
                    )
                else:
                    match = TokenGazetteerMatch(
                        idx, idx + 1, thistokens.copy(), node.data, node.listidx
                    )
                thismatches.append(match)
            j = idx + 1  # index into text tokens
            nignored = 0
            while j &lt; endidx:
                # print(f&#34;!!! processing idx={j}/{endidx}&#34;)
                if node.nodes:
                    token = tokens[j]
                    if token.type == self.splittype:
                        break
                    token_string = self.getterfunc(token, doc=doc, feature=self.feature)
                    if token_string is None:
                        j += 1
                        nignored += 1
                        continue
                    if self.mapfunc:
                        token_string = self.mapfunc(token_string)
                    if self.ignorefunc and self.ignorefunc(token_string):
                        j += 1
                        nignored += 1
                        continue
                    if token_string in node.nodes:
                        node = node.nodes[token_string]
                        thistokens.append(token)
                        if node.is_match:
                            if matchfunc:
                                match = matchfunc(
                                    idx,
                                    idx + len(thistokens) + nignored,
                                    thistokens.copy(),
                                    node.data,
                                    node.listidx,
                                )
                            else:
                                match = TokenGazetteerMatch(
                                    idx,
                                    idx + len(thistokens) + nignored,
                                    thistokens.copy(),
                                    node.data,
                                    node.listidx,
                                )
                            # debugtxt = &#34; &#34;.join(
                            #     [doc[tokens[i]] for i in range(match.start, match.end)]
                            # )
                            # TODO: should LONGEST get calculated including ignored tokens or not?
                            if not longest_only:
                                thismatches.append(match)
                                if len(thistokens) &gt; longest:
                                    longest = len(thistokens)
                            else:
                                if len(thistokens) &gt; longest:
                                    thismatches = [match]
                                    longest = len(thistokens)
                        j += 1
                        continue
                    else:
                        break
                else:
                    break
            return thismatches, longest
        else:
            # first token did not match, nothing to be found
            return [], 0

    def find(
        self,
        tokens: List[Annotation],
        doc: Optional[Document] = None,
        longest_only: Optional[bool] = None,
        fromidx: Optional[int] = None,
        toidx: Optional[int] = None,
        endidx: Optional[int] = None,
        matchfunc: Optional[Callable] = None,
    ):
        &#34;&#34;&#34;
        Find the next match in the given index range and return a tuple with two elements: the first element
        if the list of matches, empty if no match was found, the second element is the index where the matches
        were found or None if no match was found.

        Args:
            tokens: list of tokens (must allow to fetch the ith token as tokens[i])
            doc: the document to which the tokens belong. Necessary of the underlying text is used
               for the tokens.
            longest_only: whether to return all matches or just the longest ones. If not none, overrides the
               setting from init
            fromidx: first index where a match may start
            toidx: last index where a match may start
            endidx: the index in tokens after which no match must end
            matchfunc: the function to use to process each match

        Returns:
            A triple with the list of matches as the first element, the max length of matches or 0 if no matches
            as the second element and the index where the match occurs or None as the third element

        &#34;&#34;&#34;
        if longest_only is None:
            longest_only = self.longest_only
        idx = fromidx
        if idx is None:
            idx = 0
        if toidx is None:
            toidx = len(tokens) - 1
        if endidx is None:
            endidx = len(tokens)
        while idx &lt;= toidx:
            matches, long = self.match(
                tokens, idx=idx, doc=doc, longest_only=longest_only, endidx=endidx, matchfunc=matchfunc
            )
            if long == 0:
                idx += 1
                continue
            return matches, long, idx
        return [], 0, None

    def find_all(
        self,
        tokens: List[Annotation],
        doc: Optional[Document] = None,
        longest_only: Optional[bool] = None,
        skip_longest: Optional[bool] = None,
        fromidx: Optional[int] = None,
        toidx: Optional[int] = None,
        endidx: Optional[int] = None,
        matchfunc: Optional[Callable] = None,
        # reverse=True,
    ):
        &#34;&#34;&#34;
        Find gazetteer entries in a sequence of tokens.
        Note: if fromidx or toidx are bigger than the length of the tokens allows, this is silently
        ignored.

        Args:
            tokens: iterable of tokens. The getter will be applied to each one and the doc to retrieve the initial
               string.
            doc: the document this should run on. Only necessary if the text to match is not retrieved from
               the token annotation, but from the underlying document text.
            longest_only: whether to return only the longest or all matches. If not None, overrides the init
               setting
            skip_longest: skip forward over longest match (do not return contained/overlapping matches). If not
               None overrides the init setting.
            fromidx: index where to start finding in tokens
            toidx: index where to stop finding in tokens (this is the last index actually used)
            endidx: index beyond which no matches should end
            matchfunc: a function which takes the data from the gazetteer, the token and doc and performs
                some action.

        Yields:
            list of matches
        &#34;&#34;&#34;
        if longest_only is None:
            longest_only = self.longest_only
        if skip_longest is None:
            skip_longest = self.skip
        matches = []
        lentok = len(tokens)
        if endidx is None:
            endidx = lentok
        if fromidx is None:
            fromidx = 0
        if toidx is None:
            toidx = lentok - 1
        if fromidx &gt;= lentok:
            yield matches
            return
        if toidx &gt;= lentok:
            toidx = lentok - 1
        if fromidx &gt; toidx:
            yield matches
            return
        idx = fromidx
        while idx &lt;= toidx:
            matches, maxlen, idx = self.find(
                tokens,
                doc=doc,
                longest_only=longest_only,
                fromidx=idx,
                endidx=endidx,
                toidx=toidx,
                matchfunc=matchfunc,
            )
            if idx is None:
                return
            yield matches
            if skip_longest:
                idx += maxlen
            else:
                idx += 1

    def __call__(self, doc: Document, **kwargs) -&gt; Document:
        &#34;&#34;&#34;
        Apply the gazetteer to the document and annotate all matches.

        Args:
            doc: the document to annotate with matches.

        Returns:
            the annotated document
        &#34;&#34;&#34;
        # create the token lists from the document: if withintype is None we only have one token list,
        # otherwise we have one list for each withingtype
        # We create a list of segments which are identified by start and end offsets
        if self.withintype is None:
            segment_offs = [(0, len(doc.text))]
        else:
            withinanns = doc.annset(self.withintype)
            segment_offs = []
            for wann in withinanns:
                segment_offs.append((wann.start, wann.end))
        anntypes = [self.tokentype]
        if self.splittype is not None:
            anntypes.append(self.splittype)
        anns = doc.annset(self.annset).with_type(anntypes)
        # now do the annotation process for each segment
        outset = doc.annset(self.outset)
        for segment_start, segment_end in segment_offs:
            tokens = list(anns.within(segment_start, segment_end))
            for matches in self.find_all(tokens, doc=doc):
                for match in matches:
                    starttoken = tokens[match.start]
                    endtoken = tokens[
                        match.end - 1
                    ]  # end is the index after the last match!!
                    startoffset = starttoken.start
                    endoffset = endtoken.end
                    if match.data:  # TODO: for now data and listidx are either both None or lists with same len
                        for data, listidx in zip(match.data, match.listidx):
                            outtype = self.outtype
                            feats = {}
                            if listidx is not None:
                                feats.update(self.listfeatures[listidx])
                                outtype = self.listtypes[listidx]
                            if &#34;_gatenlp.gazetteer.outtype&#34; in feats:
                                outtype = feats[&#34;_gatenlp.gazetteer.outtype&#34;]
                                del feats[&#34;_gatenlp.gazetteer.outtype&#34;]
                            if data is not None:
                                feats.update(data)
                            outset.add(startoffset, endoffset, outtype, features=feats)
                    else:
                        outset.add(startoffset, endoffset, self.outtype)
        return doc

    def get(self, tokenstrings, default=None):
        if isinstance(tokenstrings, str):
            tokenstrings = [tokenstrings]
        node = self.nodes
        for idx, tokenstring in enumerate(tokenstrings):
            if idx == 0:
                node = node.get(tokenstring)   # get from defaultdict
            else:
                node = node.nodes.get(tokenstring)   # get from TokenGazetteerNode nodes
            if node is None:
                return None
        if node.is_match:
            ret = []
            assert len(node.data) == len(node.listidx)
            for d, i in zip(node.data, node.listidx):
                new = d.copy()
                new.update(self.listfeatures[i])
                ret.append(new)
            return ret
        else:
            return default

    def __getitem__(self, tokenstrings):
        ret = self.get(tokenstrings)
        if ret is None:
            raise KeyError(tokenstrings)
        return ret

    def __contains__(self, tokenstrings):
        ret = self.get(tokenstrings)
        return ret is not None

    def __len__(self):
        return self.size</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="gatenlp.processing.gazetteer.tokengazetteer.tokentext_getter"><code class="name flex">
<span>def <span class="ident">tokentext_getter</span></span>(<span>token, doc=None, feature=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def tokentext_getter(token, doc=None, feature=None):
    if feature is not None:
        txt = token.features.get(feature)
    else:
        if doc is None:
            raise Exception(&#34;No feature given, need doc for gazetteer&#34;)
        txt = doc[token]
    return txt</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="gatenlp.processing.gazetteer.tokengazetteer.TokenGazetteer"><code class="flex name class">
<span>class <span class="ident">TokenGazetteer</span></span>
<span>(</span><span>source: Union[List[~T], str, None] = None, source_fmt: str = 'gate-def', source_sep='\t', source_encoding='UTF-8', source_tokenizer: Union[None, <a title="gatenlp.processing.annotator.Annotator" href="../annotator.html#gatenlp.processing.annotator.Annotator">Annotator</a>, Callable] = None, longest_only: bool = False, skip_longest: bool = False, outset_name: str = '', ann_type: str = 'Lookup', annset_name: str = '', token_type: str = 'Token', feature=None, split_type: Optional[str] = None, within_type: Optional[str] = None, mapfunc: Optional[Callable] = None, ignorefunc: Optional[Callable] = None, getterfunc: Optional[Callable] = None, list_features: Optional[Dict[~KT, ~VT]] = None, list_type: Optional[str] = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Helper class that provides a standard way to create an ABC using
inheritance.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>source</code></strong></dt>
<dd>where to load the gazetteer from. What is actually expected here depends on the fmt
parameter. If none, nothing is loaded</dd>
<dt><strong><code>source_fmt</code></strong></dt>
<dd>defines what is expected as the format and/or content of the source parameter. One of:
*
"gate-def" (default): the path to a GATE-style "def" file.
See <a href="https://gate.ac.uk/userguide/chap:gazetteers">https://gate.ac.uk/userguide/chap:gazetteers</a>
* "gazlist": a list of tuples or lists where the first element of the tuple/list
is a list of strings and the second element is a dictionary containing the features to assign.
All entries in the list belong to the first gazetteer list which has list features as
specified with the listfeatures parameter and a list type as specified with the listtype parameter.</dd>
<dt><strong><code>source_sep</code></strong></dt>
<dd>the field separator to use for some source formats (default: tab character)</dd>
<dt><strong><code>source_encoding</code></strong></dt>
<dd>the encoding to use for some source formats (default: UTF-8)</dd>
<dt><strong><code>source_tokenizer</code></strong></dt>
<dd>if not None, an annotator, that creates annotations of type "Token" in the default
annotation set. If this is None, then when loading string gazetteer entries, they are tokenized by
splitting on whitespace (as defined by Python str.split())</dd>
<dt><strong><code>feature</code></strong></dt>
<dd>the feature name to use to get the string for each token. If the corresponding feature
in the token does not exist, is None or is the empty string, the Token is completely ignored.
If the feature parameter is None, use the document string covered by the token.</dd>
<dt><strong><code>longest_only</code></strong></dt>
<dd>if True, only returns the longest match at each matching position, otherwise returns all
matches.</dd>
<dt><strong><code>skip_longest</code></strong></dt>
<dd>skip forward over longest match (do not return contained/overlapping matches)</dd>
<dt><strong><code>annset_name</code></strong></dt>
<dd>the set where the tokens to match should come from</dd>
<dt><strong><code>outset_name</code></strong></dt>
<dd>the set where the new annotations are added</dd>
<dt><strong><code>ann_type</code></strong></dt>
<dd>the annotation type of the annotations to create, unless a type is given for the gazetteer
entry or for the gazetteer list.</dd>
<dt><strong><code>token_type</code></strong></dt>
<dd>the annotation type of the token annotations</dd>
<dt><strong><code>split_type</code></strong></dt>
<dd>the annotation type of any split annotations which will end any ongoing match</dd>
<dt><strong><code>within_type</code></strong></dt>
<dd>only matches fully within annotations of this type will be made</dd>
<dt><strong><code>mapfunc</code></strong></dt>
<dd>a callable that maps the original string extracted for each token to the actual string to use.</dd>
<dt><strong><code>ignorefunc</code></strong></dt>
<dd>a callable which given the mapped token string decides if the token should be ignored
(not added to the gazetteer list, not considered in the document when matching)</dd>
<dt><strong><code>getterfunc</code></strong></dt>
<dd>a callable which, given a token annotation, retrieves the string. If there is mapfunc, the
retrieved string is then still run through the mapfunc. The getterfunc must accept the token and
an optional document as parameters.</dd>
<dt><strong><code>list_features</code></strong></dt>
<dd>a dictionary of features common to the whole list loaded or None.
If what gets loaded specifies
its own list features, this is getting ignored.</dd>
<dt><strong><code>list_type</code></strong></dt>
<dd>the output annotation type to use for the list, ignored if the input format specifies this
on its own. If the input does not specify this on its own and this is not None, then it takes
precedence over outtype for the data loaded from source.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class TokenGazetteer(GazetteerBase):
    def __init__(
        self,
        source: Union[List, str, None] = None,
        source_fmt: str = &#34;gate-def&#34;,
        source_sep=&#34;\t&#34;,
        source_encoding=&#34;UTF-8&#34;,
        # cache_source=None,   # TODO
        source_tokenizer: Union[None, Annotator, Callable] = None,
        longest_only: bool = False,
        skip_longest: bool = False,
        outset_name: str = &#34;&#34;,
        ann_type: str = &#34;Lookup&#34;,
        annset_name: str = &#34;&#34;,
        token_type: str = &#34;Token&#34;,
        feature=None,
        split_type: Optional[str] = None,
        within_type: Optional[str] = None,
        mapfunc: Optional[Callable] = None,
        ignorefunc: Optional[Callable] = None,
        getterfunc: Optional[Callable] = None,
        list_features: Optional[Dict] = None,
        list_type: Optional[str] = None,
    ):
        &#34;&#34;&#34;

        Args:
            source: where to load the gazetteer from. What is actually expected here depends on the fmt
              parameter. If none, nothing is loaded
            source_fmt: defines what is expected as the format and/or content of the source parameter. One of:
               *  &#34;gate-def&#34; (default): the path to a GATE-style &#34;def&#34; file.
                  See https://gate.ac.uk/userguide/chap:gazetteers
               * &#34;gazlist&#34;: a list of tuples or lists where the first element of the tuple/list
                  is a list of strings and the second element is a dictionary containing the features to assign.
                  All entries in the list belong to the first gazetteer list which has list features as
                  specified with the listfeatures parameter and a list type as specified with the listtype parameter.
            source_sep: the field separator to use for some source formats (default: tab character)
            source_encoding: the encoding to use for some source formats (default: UTF-8)
            source_tokenizer: if not None, an annotator, that creates annotations of type &#34;Token&#34; in the default
                annotation set. If this is None, then when loading string gazetteer entries, they are tokenized by
                splitting on whitespace (as defined by Python str.split())
            feature: the feature name to use to get the string for each token. If the corresponding feature
                in the token does not exist, is None or is the empty string, the Token is completely ignored.
                If the feature parameter is None, use the document string covered by the token.
            longest_only: if True, only returns the longest match at each matching position, otherwise returns all
                matches.
            skip_longest: skip forward over longest match (do not return contained/overlapping matches)
            annset_name: the set where the tokens to match should come from
            outset_name: the set where the new annotations are added
            ann_type: the annotation type of the annotations to create, unless a type is given for the gazetteer
                entry or for the gazetteer list.
            token_type: the annotation type of the token annotations
            split_type: the annotation type of any split annotations which will end any ongoing match
            within_type: only matches fully within annotations of this type will be made
            mapfunc: a callable that maps the original string extracted for each token to the actual string to use.
            ignorefunc: a callable which given the mapped token string decides if the token should be ignored
                (not added to the gazetteer list, not considered in the document when matching)
            getterfunc: a callable which, given a token annotation, retrieves the string. If there is mapfunc, the
                retrieved string is then still run through the mapfunc. The getterfunc must accept the token and
                an optional document as parameters.
            list_features: a dictionary of features common to the whole list loaded or None.
                If what gets loaded specifies
                its own list features, this is getting ignored.
            list_type: the output annotation type to use for the list, ignored if the input format specifies this
                on its own. If the input does not specify this on its own and this is not None, then it takes
                precedence over outtype for the data loaded from source.

        &#34;&#34;&#34;
        self.nodes = defaultdict(TokenGazetteerNode)
        self.mapfunc = mapfunc
        self.ignorefunc = ignorefunc
        self.feature = feature
        self.annset = annset_name
        self.tokentype = token_type
        self.splittype = split_type
        self.withintype = within_type
        self.outset = outset_name
        self.outtype = ann_type
        self.longest_only = longest_only
        self.skip = skip_longest
        if getterfunc:
            self.getterfunc = getterfunc
        else:
            self.getterfunc = tokentext_getter
        self.listfeatures = []
        self.listtypes = []
        self.logger = init_logger(__name__)
        # self.logger.setLevel(logging.DEBUG)
        self.size = 0
        if source is not None:
            self.append(source, source_fmt=source_fmt,
                        list_features=list_features, list_type=list_type, source_sep=source_sep,
                        source_encoding=source_encoding, source_tokenizer=source_tokenizer
                        )

    def append(
        self,
        source:  Union[None, str, List],
        source_fmt: str = &#34;gate-def&#34;,
        source_sep: str = &#34;\t&#34;,
        source_encoding: str = &#34;UTF-8&#34;,
        source_tokenizer: Union[None, Annotator, Callable] = None,
        source_splitter: Optional[Callable] = None,
        list_features: Optional[Dict] = None,
        list_type: Optional[str] = None,
    ):
        &#34;&#34;&#34;
        This method appends more entries to gazetteer.

        Args:
            source: where to load the gazetteer from. What is actually expected here depends on the fmt
              parameter.
            source_fmt: defines what is expected as the format and/or content of the source parameter. One of:
               *  &#34;gate-def&#34; (default): source must be a string, a pathlib Path or a parsed urllib url and
                  point to a GATE-style &#34;def&#34; file. See https://gate.ac.uk/userguide/chap:gazetteers
               * &#34;gazlist&#34;: a list of tuples or lists where the first element of the tuple/list
                  is a list of strings, the second element is a dictionary containing the features to assign and
                  the third element, if it exists, is the index of an element in the listfeatures array.
            source_sep: the field separator to use for some source formats (default: tab character)
            source_encoding: the encoding to use for some source formats (default: UTF-8)
            source_tokenizer: if not None, an annotator, that creates annotations of type &#34;Token&#34; in the default
                annotation set. If this is None, then when loading string gazetteer entries, they are tokenized by
                splitting on whitespace (as defined by Python str.split())
            source_splitter: if not None and source_tokenizer is None, a callable that takes a string and returns
                the tokenstrings to use
            list_features: a list of dictionaries containing the features to set for all matches witch have the
              list index set, this list gets appended to the existing listfeatures. If what gets appended specifies
              its own list features, this is ignored.
            list_type: the output annotation type to use for the list that gets appended. If what gets appended
               specifies its own list type or list types, this is ignored.
        &#34;&#34;&#34;
        if source_fmt == &#34;gazlist&#34;:
            if list_features is not None:
                self.listfeatures.append(list_features)
            else:
                self.listfeatures.append({})
            if list_type is not None:
                self.listtypes.append(list_type)
            else:
                self.listtypes.append(self.outtype)
            listidx = len(self.listfeatures) - 1
            for el in source:
                entry = el[0]
                data = el[1]
                self.add(entry, data, listidx=listidx)
        elif source_fmt == &#34;gate-def&#34;:
            if list_features is None:
                list_features = {}
            if list_type is None:
                list_type = self.outtype
            with open(source, &#34;rt&#34;, encoding=source_encoding) as infp:
                for line in infp:
                    line = line.rstrip(&#34;\n\r&#34;)
                    fields = line.split(&#34;:&#34;)
                    fields.extend([&#34;&#34;, &#34;&#34;, &#34;&#34;, &#34;&#34;])
                    listFile = fields[0]
                    majorType = fields[1]
                    minorType = fields[2]
                    languages = fields[3]
                    anntype = fields[4]
                    this_listfeatures = list_features.copy()
                    this_outtype = list_type
                    if majorType:
                        this_listfeatures[&#34;majorType&#34;] = majorType
                    if minorType:
                        this_listfeatures[&#34;minorType&#34;] = minorType
                    if languages:
                        this_listfeatures[&#34;lang&#34;] = languages
                    if anntype:
                        this_outtype = anntype
                    # read in the actual list
                    listfile = os.path.join(os.path.dirname(source), listFile)
                    self.logger.debug(f&#34;Reading list file {listfile}&#34;)
                    with open(listfile, &#34;rt&#34;, encoding=source_encoding) as inlistfile:
                        self.listtypes.append(this_outtype)
                        self.listfeatures.append(this_listfeatures)
                        linenr = 0
                        for listline in inlistfile:
                            linenr += 1
                            listline = listline.rstrip(&#34;\n\r&#34;)
                            fields = listline.split(source_sep)
                            entry = fields[0]
                            if source_tokenizer or source_splitter:
                                if source_tokenizer:
                                    tmpdoc = Document(entry)
                                    tmpdoc = source_tokenizer(tmpdoc)  # we MUST reassign here to allow return of a new doc!
                                    tokenanns = list(tmpdoc.annset().with_type(&#34;Token&#34;))
                                    if self.getterfunc:
                                        tokenstrings = [
                                            self.getterfunc(a, doc=tmpdoc)
                                            for a in tokenanns
                                        ]
                                    else:
                                        tokenstrings = [tmpdoc[a] for a in tokenanns]
                                else:
                                    tokenstrings = source_splitter(entry)
                                if self.mapfunc:
                                    tokenstrings = [
                                        self.mapfunc(s) for s in tokenstrings
                                    ]
                                if self.ignorefunc:
                                    tokenstrings = [
                                        s
                                        for s in tokenstrings
                                        if not self.ignorefunc(s)
                                    ]
                            else:
                                tokenstrings = entry.split()  # just split on whitespace
                            if len(tokenstrings) == 0:
                                self.logger.warning(
                                    f&#34;File {listfile}, skipping line {linenr}, no tokens left: {listline}&#34;
                                )
                                continue
                            if len(entry) &gt; 1:
                                feats = {}
                                for fspec in fields[1:]:
                                    fname, fval = fspec.split(&#34;=&#34;)
                                    feats[fname] = fval
                            else:
                                feats = None
                            listidx = len(self.listfeatures) - 1
                            self.add(tokenstrings, feats, listidx=listidx)
        else:
            raise Exception(f&#34;TokenGazetteer format {source_fmt} not known&#34;)

    def add(self, entry, data=None, listidx=None):
        &#34;&#34;&#34;
        Add a single gazetteer entry. A gazetteer entry can have no data associated with it at all if both
        data and listidx are None. If only list indices are given then an array of those indices is stored
        with the entry and data remaines None, if only data is given then an array of data is stored and
        listidx remains None. If at some point, both data and a listidx are stored in the same entry, then
        both fields are changed to have both a list with the same number of elements corresponding to each
        other, with missing data or listidx elements being None.

        Args:
            entry: a iterable of string or a string for a single element, each element is the string that
               represents a token to be matched
            data: dictionary of features to add
            listidx: the index to list features and a list type to add
        &#34;&#34;&#34;
        if isinstance(entry, str):
            entry = [entry]
        node = None
        i = 0
        for token in entry:
            if self.mapfunc is not None:
                token = self.mapfunc(token)
            if self.ignorefunc is not None and self.ignorefunc(token):
                continue
            if i == 0:
                node = self.nodes[token]
            else:
                if node.nodes is None:
                    node.nodes = defaultdict(TokenGazetteerNode)
                    tmpnode = TokenGazetteerNode()
                    node.nodes[token] = tmpnode
                    node = tmpnode
                else:
                    node = node.nodes[token]
            i += 1
        node.is_match = True
        self.size += 1
        # For now: always store parallel lists of data and listidxs, with None elements if necessary.
        if data is not None or listidx is not None:
            if node.data is None:
                node.data = [data]
                node.listidx = [listidx]
            else:
                node.data.append(data)
                node.listidx.append(listidx)

        # TODO: code to test and correct: try to save space by only storing parallel lists if
        # both data and listindices are actually both non-null and added:
        #
        # if data is None and listidx is None:
        #     # nothing to do, return what we have
        #     return node.data, node.listidx
        # # if we have only data and no listidx and there is no listidx
        # if data is not None and listidx is None and node.listidx is None:
        #     if node.data is None:
        #         node.data = [data]
        #     else:
        #         node.data.append(data)
        # elif listidx is not None and data is None and node.data is None:
        #     if node.listidx is None:
        #         node.listidx = [listidx]
        #     else:
        #         node.listidx.append(listidx)
        # else:
        #     # make sure we have parallel lists
        #     if node.data is None:
        #         node.data = []
        #     if node.listidx is None:
        #         node.listidx = []
        #     if len(node.data) &gt; len(node.listidx):
        #         node.listidx.extend([None] * (len(node.data) - len(node.listidx)))
        #     elif len(node.listidx) &gt; len(node.data):
        #         node.data.extend([None] * (len(node.listidx) - len(node.data)))
        #     if listidx:
        #         node.listidx.append(listidx)
        #         if data:
        #             node.data.append(data)
        #         else:
        #             node.data.append(None)
        #     else:
        #         node.listidx.append(None)
        #         node.listidx.append(listidx)

    def match(self, tokens, doc=None, longest_only=None, idx=0, endidx=None, matchfunc=None):
        &#34;&#34;&#34;
        Try to match at index location idx of the tokens sequence. Returns a list which contains
        no elements if no match is found,  or
        as many elements as matches are found. The element for each match is either a
        TokenGazeteerMatch instance if matchfunc is None or whatever matchfunc returns for a match.
        Also returns the legngth of the longest match (0 if no match).

        Args:
            tokens: a list of tokens (must allow to fetch the ith token as tokens[i])
            doc: the document to which the tokens belong. Necessary of the underlying text is used
               for the tokens.
            longest_only: whether to return all matches or just the longest ones. If None, overrides the setting
               from init.
            idx: the index in tokens where the match must start
            endidx: the index in tokens after which no match must end
            matchfunc: a function to process each match.
               The function is passed the TokenGazetteerMatch and the doc and should return something
               that is then added to the result list of matches.

        Returns:
            A tuple, where the first element is a list of match elements, empty if no matches are found
            and the second element is the length of the longest match, 0 if no match.

        &#34;&#34;&#34;
        if endidx is None:
            endidx = len(tokens)
        assert idx &lt; endidx
        if longest_only is None:
            longest_only = self.longest_only
        token = tokens[idx]
        if token.type == self.splittype:
            return [], 0
        token_string = self.getterfunc(token, doc=doc, feature=self.feature)
        if token_string is None:
            return [], 0
        if self.mapfunc:
            token_string = self.mapfunc(token_string)
        if self.ignorefunc:
            if self.ignorefunc(token_string):
                # no match possible here
                return [], 0
        # check if we can match the current token
        if token_string in self.nodes:
            # ok, we have the beginning of a possible match
            longest = 0
            node = self.nodes[token_string]
            thismatches = []
            thistokens = [token]
            if node.is_match:
                # the first token is already a complete match, so we need to add this to thismatches
                longest = 1
                # TODO: make this work with list data!
                if matchfunc:
                    match = matchfunc(
                        idx, idx + 1, thistokens.copy(), node.data, node.listidx
                    )
                else:
                    match = TokenGazetteerMatch(
                        idx, idx + 1, thistokens.copy(), node.data, node.listidx
                    )
                thismatches.append(match)
            j = idx + 1  # index into text tokens
            nignored = 0
            while j &lt; endidx:
                # print(f&#34;!!! processing idx={j}/{endidx}&#34;)
                if node.nodes:
                    token = tokens[j]
                    if token.type == self.splittype:
                        break
                    token_string = self.getterfunc(token, doc=doc, feature=self.feature)
                    if token_string is None:
                        j += 1
                        nignored += 1
                        continue
                    if self.mapfunc:
                        token_string = self.mapfunc(token_string)
                    if self.ignorefunc and self.ignorefunc(token_string):
                        j += 1
                        nignored += 1
                        continue
                    if token_string in node.nodes:
                        node = node.nodes[token_string]
                        thistokens.append(token)
                        if node.is_match:
                            if matchfunc:
                                match = matchfunc(
                                    idx,
                                    idx + len(thistokens) + nignored,
                                    thistokens.copy(),
                                    node.data,
                                    node.listidx,
                                )
                            else:
                                match = TokenGazetteerMatch(
                                    idx,
                                    idx + len(thistokens) + nignored,
                                    thistokens.copy(),
                                    node.data,
                                    node.listidx,
                                )
                            # debugtxt = &#34; &#34;.join(
                            #     [doc[tokens[i]] for i in range(match.start, match.end)]
                            # )
                            # TODO: should LONGEST get calculated including ignored tokens or not?
                            if not longest_only:
                                thismatches.append(match)
                                if len(thistokens) &gt; longest:
                                    longest = len(thistokens)
                            else:
                                if len(thistokens) &gt; longest:
                                    thismatches = [match]
                                    longest = len(thistokens)
                        j += 1
                        continue
                    else:
                        break
                else:
                    break
            return thismatches, longest
        else:
            # first token did not match, nothing to be found
            return [], 0

    def find(
        self,
        tokens: List[Annotation],
        doc: Optional[Document] = None,
        longest_only: Optional[bool] = None,
        fromidx: Optional[int] = None,
        toidx: Optional[int] = None,
        endidx: Optional[int] = None,
        matchfunc: Optional[Callable] = None,
    ):
        &#34;&#34;&#34;
        Find the next match in the given index range and return a tuple with two elements: the first element
        if the list of matches, empty if no match was found, the second element is the index where the matches
        were found or None if no match was found.

        Args:
            tokens: list of tokens (must allow to fetch the ith token as tokens[i])
            doc: the document to which the tokens belong. Necessary of the underlying text is used
               for the tokens.
            longest_only: whether to return all matches or just the longest ones. If not none, overrides the
               setting from init
            fromidx: first index where a match may start
            toidx: last index where a match may start
            endidx: the index in tokens after which no match must end
            matchfunc: the function to use to process each match

        Returns:
            A triple with the list of matches as the first element, the max length of matches or 0 if no matches
            as the second element and the index where the match occurs or None as the third element

        &#34;&#34;&#34;
        if longest_only is None:
            longest_only = self.longest_only
        idx = fromidx
        if idx is None:
            idx = 0
        if toidx is None:
            toidx = len(tokens) - 1
        if endidx is None:
            endidx = len(tokens)
        while idx &lt;= toidx:
            matches, long = self.match(
                tokens, idx=idx, doc=doc, longest_only=longest_only, endidx=endidx, matchfunc=matchfunc
            )
            if long == 0:
                idx += 1
                continue
            return matches, long, idx
        return [], 0, None

    def find_all(
        self,
        tokens: List[Annotation],
        doc: Optional[Document] = None,
        longest_only: Optional[bool] = None,
        skip_longest: Optional[bool] = None,
        fromidx: Optional[int] = None,
        toidx: Optional[int] = None,
        endidx: Optional[int] = None,
        matchfunc: Optional[Callable] = None,
        # reverse=True,
    ):
        &#34;&#34;&#34;
        Find gazetteer entries in a sequence of tokens.
        Note: if fromidx or toidx are bigger than the length of the tokens allows, this is silently
        ignored.

        Args:
            tokens: iterable of tokens. The getter will be applied to each one and the doc to retrieve the initial
               string.
            doc: the document this should run on. Only necessary if the text to match is not retrieved from
               the token annotation, but from the underlying document text.
            longest_only: whether to return only the longest or all matches. If not None, overrides the init
               setting
            skip_longest: skip forward over longest match (do not return contained/overlapping matches). If not
               None overrides the init setting.
            fromidx: index where to start finding in tokens
            toidx: index where to stop finding in tokens (this is the last index actually used)
            endidx: index beyond which no matches should end
            matchfunc: a function which takes the data from the gazetteer, the token and doc and performs
                some action.

        Yields:
            list of matches
        &#34;&#34;&#34;
        if longest_only is None:
            longest_only = self.longest_only
        if skip_longest is None:
            skip_longest = self.skip
        matches = []
        lentok = len(tokens)
        if endidx is None:
            endidx = lentok
        if fromidx is None:
            fromidx = 0
        if toidx is None:
            toidx = lentok - 1
        if fromidx &gt;= lentok:
            yield matches
            return
        if toidx &gt;= lentok:
            toidx = lentok - 1
        if fromidx &gt; toidx:
            yield matches
            return
        idx = fromidx
        while idx &lt;= toidx:
            matches, maxlen, idx = self.find(
                tokens,
                doc=doc,
                longest_only=longest_only,
                fromidx=idx,
                endidx=endidx,
                toidx=toidx,
                matchfunc=matchfunc,
            )
            if idx is None:
                return
            yield matches
            if skip_longest:
                idx += maxlen
            else:
                idx += 1

    def __call__(self, doc: Document, **kwargs) -&gt; Document:
        &#34;&#34;&#34;
        Apply the gazetteer to the document and annotate all matches.

        Args:
            doc: the document to annotate with matches.

        Returns:
            the annotated document
        &#34;&#34;&#34;
        # create the token lists from the document: if withintype is None we only have one token list,
        # otherwise we have one list for each withingtype
        # We create a list of segments which are identified by start and end offsets
        if self.withintype is None:
            segment_offs = [(0, len(doc.text))]
        else:
            withinanns = doc.annset(self.withintype)
            segment_offs = []
            for wann in withinanns:
                segment_offs.append((wann.start, wann.end))
        anntypes = [self.tokentype]
        if self.splittype is not None:
            anntypes.append(self.splittype)
        anns = doc.annset(self.annset).with_type(anntypes)
        # now do the annotation process for each segment
        outset = doc.annset(self.outset)
        for segment_start, segment_end in segment_offs:
            tokens = list(anns.within(segment_start, segment_end))
            for matches in self.find_all(tokens, doc=doc):
                for match in matches:
                    starttoken = tokens[match.start]
                    endtoken = tokens[
                        match.end - 1
                    ]  # end is the index after the last match!!
                    startoffset = starttoken.start
                    endoffset = endtoken.end
                    if match.data:  # TODO: for now data and listidx are either both None or lists with same len
                        for data, listidx in zip(match.data, match.listidx):
                            outtype = self.outtype
                            feats = {}
                            if listidx is not None:
                                feats.update(self.listfeatures[listidx])
                                outtype = self.listtypes[listidx]
                            if &#34;_gatenlp.gazetteer.outtype&#34; in feats:
                                outtype = feats[&#34;_gatenlp.gazetteer.outtype&#34;]
                                del feats[&#34;_gatenlp.gazetteer.outtype&#34;]
                            if data is not None:
                                feats.update(data)
                            outset.add(startoffset, endoffset, outtype, features=feats)
                    else:
                        outset.add(startoffset, endoffset, self.outtype)
        return doc

    def get(self, tokenstrings, default=None):
        if isinstance(tokenstrings, str):
            tokenstrings = [tokenstrings]
        node = self.nodes
        for idx, tokenstring in enumerate(tokenstrings):
            if idx == 0:
                node = node.get(tokenstring)   # get from defaultdict
            else:
                node = node.nodes.get(tokenstring)   # get from TokenGazetteerNode nodes
            if node is None:
                return None
        if node.is_match:
            ret = []
            assert len(node.data) == len(node.listidx)
            for d, i in zip(node.data, node.listidx):
                new = d.copy()
                new.update(self.listfeatures[i])
                ret.append(new)
            return ret
        else:
            return default

    def __getitem__(self, tokenstrings):
        ret = self.get(tokenstrings)
        if ret is None:
            raise KeyError(tokenstrings)
        return ret

    def __contains__(self, tokenstrings):
        ret = self.get(tokenstrings)
        return ret is not None

    def __len__(self):
        return self.size</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="gatenlp.processing.gazetteer.base.GazetteerBase" href="base.html#gatenlp.processing.gazetteer.base.GazetteerBase">GazetteerBase</a></li>
<li><a title="gatenlp.processing.annotator.Annotator" href="../annotator.html#gatenlp.processing.annotator.Annotator">Annotator</a></li>
<li>abc.ABC</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="gatenlp.processing.gazetteer.tokengazetteer.TokenGazetteer.add"><code class="name flex">
<span>def <span class="ident">add</span></span>(<span>self, entry, data=None, listidx=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Add a single gazetteer entry. A gazetteer entry can have no data associated with it at all if both
data and listidx are None. If only list indices are given then an array of those indices is stored
with the entry and data remaines None, if only data is given then an array of data is stored and
listidx remains None. If at some point, both data and a listidx are stored in the same entry, then
both fields are changed to have both a list with the same number of elements corresponding to each
other, with missing data or listidx elements being None.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>entry</code></strong></dt>
<dd>a iterable of string or a string for a single element, each element is the string that
represents a token to be matched</dd>
<dt><strong><code>data</code></strong></dt>
<dd>dictionary of features to add</dd>
<dt><strong><code>listidx</code></strong></dt>
<dd>the index to list features and a list type to add</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def add(self, entry, data=None, listidx=None):
    &#34;&#34;&#34;
    Add a single gazetteer entry. A gazetteer entry can have no data associated with it at all if both
    data and listidx are None. If only list indices are given then an array of those indices is stored
    with the entry and data remaines None, if only data is given then an array of data is stored and
    listidx remains None. If at some point, both data and a listidx are stored in the same entry, then
    both fields are changed to have both a list with the same number of elements corresponding to each
    other, with missing data or listidx elements being None.

    Args:
        entry: a iterable of string or a string for a single element, each element is the string that
           represents a token to be matched
        data: dictionary of features to add
        listidx: the index to list features and a list type to add
    &#34;&#34;&#34;
    if isinstance(entry, str):
        entry = [entry]
    node = None
    i = 0
    for token in entry:
        if self.mapfunc is not None:
            token = self.mapfunc(token)
        if self.ignorefunc is not None and self.ignorefunc(token):
            continue
        if i == 0:
            node = self.nodes[token]
        else:
            if node.nodes is None:
                node.nodes = defaultdict(TokenGazetteerNode)
                tmpnode = TokenGazetteerNode()
                node.nodes[token] = tmpnode
                node = tmpnode
            else:
                node = node.nodes[token]
        i += 1
    node.is_match = True
    self.size += 1
    # For now: always store parallel lists of data and listidxs, with None elements if necessary.
    if data is not None or listidx is not None:
        if node.data is None:
            node.data = [data]
            node.listidx = [listidx]
        else:
            node.data.append(data)
            node.listidx.append(listidx)</code></pre>
</details>
</dd>
<dt id="gatenlp.processing.gazetteer.tokengazetteer.TokenGazetteer.append"><code class="name flex">
<span>def <span class="ident">append</span></span>(<span>self, source: Union[List[~T], str, None], source_fmt: str = 'gate-def', source_sep: str = '\t', source_encoding: str = 'UTF-8', source_tokenizer: Union[None, <a title="gatenlp.processing.annotator.Annotator" href="../annotator.html#gatenlp.processing.annotator.Annotator">Annotator</a>, Callable] = None, source_splitter: Optional[Callable] = None, list_features: Optional[Dict[~KT, ~VT]] = None, list_type: Optional[str] = None)</span>
</code></dt>
<dd>
<div class="desc"><p>This method appends more entries to gazetteer.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>source</code></strong></dt>
<dd>where to load the gazetteer from. What is actually expected here depends on the fmt
parameter.</dd>
<dt><strong><code>source_fmt</code></strong></dt>
<dd>defines what is expected as the format and/or content of the source parameter. One of:
*
"gate-def" (default): source must be a string, a pathlib Path or a parsed urllib url and
point to a GATE-style "def" file. See <a href="https://gate.ac.uk/userguide/chap:gazetteers">https://gate.ac.uk/userguide/chap:gazetteers</a>
* "gazlist": a list of tuples or lists where the first element of the tuple/list
is a list of strings, the second element is a dictionary containing the features to assign and
the third element, if it exists, is the index of an element in the listfeatures array.</dd>
<dt><strong><code>source_sep</code></strong></dt>
<dd>the field separator to use for some source formats (default: tab character)</dd>
<dt><strong><code>source_encoding</code></strong></dt>
<dd>the encoding to use for some source formats (default: UTF-8)</dd>
<dt><strong><code>source_tokenizer</code></strong></dt>
<dd>if not None, an annotator, that creates annotations of type "Token" in the default
annotation set. If this is None, then when loading string gazetteer entries, they are tokenized by
splitting on whitespace (as defined by Python str.split())</dd>
<dt><strong><code>source_splitter</code></strong></dt>
<dd>if not None and source_tokenizer is None, a callable that takes a string and returns
the tokenstrings to use</dd>
<dt><strong><code>list_features</code></strong></dt>
<dd>a list of dictionaries containing the features to set for all matches witch have the
list index set, this list gets appended to the existing listfeatures. If what gets appended specifies
its own list features, this is ignored.</dd>
<dt><strong><code>list_type</code></strong></dt>
<dd>the output annotation type to use for the list that gets appended. If what gets appended
specifies its own list type or list types, this is ignored.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def append(
    self,
    source:  Union[None, str, List],
    source_fmt: str = &#34;gate-def&#34;,
    source_sep: str = &#34;\t&#34;,
    source_encoding: str = &#34;UTF-8&#34;,
    source_tokenizer: Union[None, Annotator, Callable] = None,
    source_splitter: Optional[Callable] = None,
    list_features: Optional[Dict] = None,
    list_type: Optional[str] = None,
):
    &#34;&#34;&#34;
    This method appends more entries to gazetteer.

    Args:
        source: where to load the gazetteer from. What is actually expected here depends on the fmt
          parameter.
        source_fmt: defines what is expected as the format and/or content of the source parameter. One of:
           *  &#34;gate-def&#34; (default): source must be a string, a pathlib Path or a parsed urllib url and
              point to a GATE-style &#34;def&#34; file. See https://gate.ac.uk/userguide/chap:gazetteers
           * &#34;gazlist&#34;: a list of tuples or lists where the first element of the tuple/list
              is a list of strings, the second element is a dictionary containing the features to assign and
              the third element, if it exists, is the index of an element in the listfeatures array.
        source_sep: the field separator to use for some source formats (default: tab character)
        source_encoding: the encoding to use for some source formats (default: UTF-8)
        source_tokenizer: if not None, an annotator, that creates annotations of type &#34;Token&#34; in the default
            annotation set. If this is None, then when loading string gazetteer entries, they are tokenized by
            splitting on whitespace (as defined by Python str.split())
        source_splitter: if not None and source_tokenizer is None, a callable that takes a string and returns
            the tokenstrings to use
        list_features: a list of dictionaries containing the features to set for all matches witch have the
          list index set, this list gets appended to the existing listfeatures. If what gets appended specifies
          its own list features, this is ignored.
        list_type: the output annotation type to use for the list that gets appended. If what gets appended
           specifies its own list type or list types, this is ignored.
    &#34;&#34;&#34;
    if source_fmt == &#34;gazlist&#34;:
        if list_features is not None:
            self.listfeatures.append(list_features)
        else:
            self.listfeatures.append({})
        if list_type is not None:
            self.listtypes.append(list_type)
        else:
            self.listtypes.append(self.outtype)
        listidx = len(self.listfeatures) - 1
        for el in source:
            entry = el[0]
            data = el[1]
            self.add(entry, data, listidx=listidx)
    elif source_fmt == &#34;gate-def&#34;:
        if list_features is None:
            list_features = {}
        if list_type is None:
            list_type = self.outtype
        with open(source, &#34;rt&#34;, encoding=source_encoding) as infp:
            for line in infp:
                line = line.rstrip(&#34;\n\r&#34;)
                fields = line.split(&#34;:&#34;)
                fields.extend([&#34;&#34;, &#34;&#34;, &#34;&#34;, &#34;&#34;])
                listFile = fields[0]
                majorType = fields[1]
                minorType = fields[2]
                languages = fields[3]
                anntype = fields[4]
                this_listfeatures = list_features.copy()
                this_outtype = list_type
                if majorType:
                    this_listfeatures[&#34;majorType&#34;] = majorType
                if minorType:
                    this_listfeatures[&#34;minorType&#34;] = minorType
                if languages:
                    this_listfeatures[&#34;lang&#34;] = languages
                if anntype:
                    this_outtype = anntype
                # read in the actual list
                listfile = os.path.join(os.path.dirname(source), listFile)
                self.logger.debug(f&#34;Reading list file {listfile}&#34;)
                with open(listfile, &#34;rt&#34;, encoding=source_encoding) as inlistfile:
                    self.listtypes.append(this_outtype)
                    self.listfeatures.append(this_listfeatures)
                    linenr = 0
                    for listline in inlistfile:
                        linenr += 1
                        listline = listline.rstrip(&#34;\n\r&#34;)
                        fields = listline.split(source_sep)
                        entry = fields[0]
                        if source_tokenizer or source_splitter:
                            if source_tokenizer:
                                tmpdoc = Document(entry)
                                tmpdoc = source_tokenizer(tmpdoc)  # we MUST reassign here to allow return of a new doc!
                                tokenanns = list(tmpdoc.annset().with_type(&#34;Token&#34;))
                                if self.getterfunc:
                                    tokenstrings = [
                                        self.getterfunc(a, doc=tmpdoc)
                                        for a in tokenanns
                                    ]
                                else:
                                    tokenstrings = [tmpdoc[a] for a in tokenanns]
                            else:
                                tokenstrings = source_splitter(entry)
                            if self.mapfunc:
                                tokenstrings = [
                                    self.mapfunc(s) for s in tokenstrings
                                ]
                            if self.ignorefunc:
                                tokenstrings = [
                                    s
                                    for s in tokenstrings
                                    if not self.ignorefunc(s)
                                ]
                        else:
                            tokenstrings = entry.split()  # just split on whitespace
                        if len(tokenstrings) == 0:
                            self.logger.warning(
                                f&#34;File {listfile}, skipping line {linenr}, no tokens left: {listline}&#34;
                            )
                            continue
                        if len(entry) &gt; 1:
                            feats = {}
                            for fspec in fields[1:]:
                                fname, fval = fspec.split(&#34;=&#34;)
                                feats[fname] = fval
                        else:
                            feats = None
                        listidx = len(self.listfeatures) - 1
                        self.add(tokenstrings, feats, listidx=listidx)
    else:
        raise Exception(f&#34;TokenGazetteer format {source_fmt} not known&#34;)</code></pre>
</details>
</dd>
<dt id="gatenlp.processing.gazetteer.tokengazetteer.TokenGazetteer.find"><code class="name flex">
<span>def <span class="ident">find</span></span>(<span>self, tokens: List[<a title="gatenlp.annotation.Annotation" href="../../annotation.html#gatenlp.annotation.Annotation">Annotation</a>], doc: Optional[<a title="gatenlp.document.Document" href="../../document.html#gatenlp.document.Document">Document</a>] = None, longest_only: Optional[None] = None, fromidx: Optional[int] = None, toidx: Optional[int] = None, endidx: Optional[int] = None, matchfunc: Optional[Callable] = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Find the next match in the given index range and return a tuple with two elements: the first element
if the list of matches, empty if no match was found, the second element is the index where the matches
were found or None if no match was found.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>tokens</code></strong></dt>
<dd>list of tokens (must allow to fetch the ith token as tokens[i])</dd>
<dt><strong><code>doc</code></strong></dt>
<dd>the document to which the tokens belong. Necessary of the underlying text is used
for the tokens.</dd>
<dt><strong><code>longest_only</code></strong></dt>
<dd>whether to return all matches or just the longest ones. If not none, overrides the
setting from init</dd>
<dt><strong><code>fromidx</code></strong></dt>
<dd>first index where a match may start</dd>
<dt><strong><code>toidx</code></strong></dt>
<dd>last index where a match may start</dd>
<dt><strong><code>endidx</code></strong></dt>
<dd>the index in tokens after which no match must end</dd>
<dt><strong><code>matchfunc</code></strong></dt>
<dd>the function to use to process each match</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A triple with the list of matches as the first element, the max length of matches or 0 if no matches
as the second element and the index where the match occurs or None as the third element</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def find(
    self,
    tokens: List[Annotation],
    doc: Optional[Document] = None,
    longest_only: Optional[bool] = None,
    fromidx: Optional[int] = None,
    toidx: Optional[int] = None,
    endidx: Optional[int] = None,
    matchfunc: Optional[Callable] = None,
):
    &#34;&#34;&#34;
    Find the next match in the given index range and return a tuple with two elements: the first element
    if the list of matches, empty if no match was found, the second element is the index where the matches
    were found or None if no match was found.

    Args:
        tokens: list of tokens (must allow to fetch the ith token as tokens[i])
        doc: the document to which the tokens belong. Necessary of the underlying text is used
           for the tokens.
        longest_only: whether to return all matches or just the longest ones. If not none, overrides the
           setting from init
        fromidx: first index where a match may start
        toidx: last index where a match may start
        endidx: the index in tokens after which no match must end
        matchfunc: the function to use to process each match

    Returns:
        A triple with the list of matches as the first element, the max length of matches or 0 if no matches
        as the second element and the index where the match occurs or None as the third element

    &#34;&#34;&#34;
    if longest_only is None:
        longest_only = self.longest_only
    idx = fromidx
    if idx is None:
        idx = 0
    if toidx is None:
        toidx = len(tokens) - 1
    if endidx is None:
        endidx = len(tokens)
    while idx &lt;= toidx:
        matches, long = self.match(
            tokens, idx=idx, doc=doc, longest_only=longest_only, endidx=endidx, matchfunc=matchfunc
        )
        if long == 0:
            idx += 1
            continue
        return matches, long, idx
    return [], 0, None</code></pre>
</details>
</dd>
<dt id="gatenlp.processing.gazetteer.tokengazetteer.TokenGazetteer.find_all"><code class="name flex">
<span>def <span class="ident">find_all</span></span>(<span>self, tokens: List[<a title="gatenlp.annotation.Annotation" href="../../annotation.html#gatenlp.annotation.Annotation">Annotation</a>], doc: Optional[<a title="gatenlp.document.Document" href="../../document.html#gatenlp.document.Document">Document</a>] = None, longest_only: Optional[None] = None, skip_longest: Optional[None] = None, fromidx: Optional[int] = None, toidx: Optional[int] = None, endidx: Optional[int] = None, matchfunc: Optional[Callable] = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Find gazetteer entries in a sequence of tokens.
Note: if fromidx or toidx are bigger than the length of the tokens allows, this is silently
ignored.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>tokens</code></strong></dt>
<dd>iterable of tokens. The getter will be applied to each one and the doc to retrieve the initial
string.</dd>
<dt><strong><code>doc</code></strong></dt>
<dd>the document this should run on. Only necessary if the text to match is not retrieved from
the token annotation, but from the underlying document text.</dd>
<dt><strong><code>longest_only</code></strong></dt>
<dd>whether to return only the longest or all matches. If not None, overrides the init
setting</dd>
<dt><strong><code>skip_longest</code></strong></dt>
<dd>skip forward over longest match (do not return contained/overlapping matches). If not
None overrides the init setting.</dd>
<dt><strong><code>fromidx</code></strong></dt>
<dd>index where to start finding in tokens</dd>
<dt><strong><code>toidx</code></strong></dt>
<dd>index where to stop finding in tokens (this is the last index actually used)</dd>
<dt><strong><code>endidx</code></strong></dt>
<dd>index beyond which no matches should end</dd>
<dt><strong><code>matchfunc</code></strong></dt>
<dd>a function which takes the data from the gazetteer, the token and doc and performs
some action.</dd>
</dl>
<h2 id="yields">Yields</h2>
<p>list of matches</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def find_all(
    self,
    tokens: List[Annotation],
    doc: Optional[Document] = None,
    longest_only: Optional[bool] = None,
    skip_longest: Optional[bool] = None,
    fromidx: Optional[int] = None,
    toidx: Optional[int] = None,
    endidx: Optional[int] = None,
    matchfunc: Optional[Callable] = None,
    # reverse=True,
):
    &#34;&#34;&#34;
    Find gazetteer entries in a sequence of tokens.
    Note: if fromidx or toidx are bigger than the length of the tokens allows, this is silently
    ignored.

    Args:
        tokens: iterable of tokens. The getter will be applied to each one and the doc to retrieve the initial
           string.
        doc: the document this should run on. Only necessary if the text to match is not retrieved from
           the token annotation, but from the underlying document text.
        longest_only: whether to return only the longest or all matches. If not None, overrides the init
           setting
        skip_longest: skip forward over longest match (do not return contained/overlapping matches). If not
           None overrides the init setting.
        fromidx: index where to start finding in tokens
        toidx: index where to stop finding in tokens (this is the last index actually used)
        endidx: index beyond which no matches should end
        matchfunc: a function which takes the data from the gazetteer, the token and doc and performs
            some action.

    Yields:
        list of matches
    &#34;&#34;&#34;
    if longest_only is None:
        longest_only = self.longest_only
    if skip_longest is None:
        skip_longest = self.skip
    matches = []
    lentok = len(tokens)
    if endidx is None:
        endidx = lentok
    if fromidx is None:
        fromidx = 0
    if toidx is None:
        toidx = lentok - 1
    if fromidx &gt;= lentok:
        yield matches
        return
    if toidx &gt;= lentok:
        toidx = lentok - 1
    if fromidx &gt; toidx:
        yield matches
        return
    idx = fromidx
    while idx &lt;= toidx:
        matches, maxlen, idx = self.find(
            tokens,
            doc=doc,
            longest_only=longest_only,
            fromidx=idx,
            endidx=endidx,
            toidx=toidx,
            matchfunc=matchfunc,
        )
        if idx is None:
            return
        yield matches
        if skip_longest:
            idx += maxlen
        else:
            idx += 1</code></pre>
</details>
</dd>
<dt id="gatenlp.processing.gazetteer.tokengazetteer.TokenGazetteer.get"><code class="name flex">
<span>def <span class="ident">get</span></span>(<span>self, tokenstrings, default=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get(self, tokenstrings, default=None):
    if isinstance(tokenstrings, str):
        tokenstrings = [tokenstrings]
    node = self.nodes
    for idx, tokenstring in enumerate(tokenstrings):
        if idx == 0:
            node = node.get(tokenstring)   # get from defaultdict
        else:
            node = node.nodes.get(tokenstring)   # get from TokenGazetteerNode nodes
        if node is None:
            return None
    if node.is_match:
        ret = []
        assert len(node.data) == len(node.listidx)
        for d, i in zip(node.data, node.listidx):
            new = d.copy()
            new.update(self.listfeatures[i])
            ret.append(new)
        return ret
    else:
        return default</code></pre>
</details>
</dd>
<dt id="gatenlp.processing.gazetteer.tokengazetteer.TokenGazetteer.match"><code class="name flex">
<span>def <span class="ident">match</span></span>(<span>self, tokens, doc=None, longest_only=None, idx=0, endidx=None, matchfunc=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Try to match at index location idx of the tokens sequence. Returns a list which contains
no elements if no match is found,
or
as many elements as matches are found. The element for each match is either a
TokenGazeteerMatch instance if matchfunc is None or whatever matchfunc returns for a match.
Also returns the legngth of the longest match (0 if no match).</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>tokens</code></strong></dt>
<dd>a list of tokens (must allow to fetch the ith token as tokens[i])</dd>
<dt><strong><code>doc</code></strong></dt>
<dd>the document to which the tokens belong. Necessary of the underlying text is used
for the tokens.</dd>
<dt><strong><code>longest_only</code></strong></dt>
<dd>whether to return all matches or just the longest ones. If None, overrides the setting
from init.</dd>
<dt><strong><code>idx</code></strong></dt>
<dd>the index in tokens where the match must start</dd>
<dt><strong><code>endidx</code></strong></dt>
<dd>the index in tokens after which no match must end</dd>
<dt><strong><code>matchfunc</code></strong></dt>
<dd>a function to process each match.
The function is passed the TokenGazetteerMatch and the doc and should return something
that is then added to the result list of matches.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A tuple, where the first element is a list of match elements, empty if no matches are found
and the second element is the length of the longest match, 0 if no match.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def match(self, tokens, doc=None, longest_only=None, idx=0, endidx=None, matchfunc=None):
    &#34;&#34;&#34;
    Try to match at index location idx of the tokens sequence. Returns a list which contains
    no elements if no match is found,  or
    as many elements as matches are found. The element for each match is either a
    TokenGazeteerMatch instance if matchfunc is None or whatever matchfunc returns for a match.
    Also returns the legngth of the longest match (0 if no match).

    Args:
        tokens: a list of tokens (must allow to fetch the ith token as tokens[i])
        doc: the document to which the tokens belong. Necessary of the underlying text is used
           for the tokens.
        longest_only: whether to return all matches or just the longest ones. If None, overrides the setting
           from init.
        idx: the index in tokens where the match must start
        endidx: the index in tokens after which no match must end
        matchfunc: a function to process each match.
           The function is passed the TokenGazetteerMatch and the doc and should return something
           that is then added to the result list of matches.

    Returns:
        A tuple, where the first element is a list of match elements, empty if no matches are found
        and the second element is the length of the longest match, 0 if no match.

    &#34;&#34;&#34;
    if endidx is None:
        endidx = len(tokens)
    assert idx &lt; endidx
    if longest_only is None:
        longest_only = self.longest_only
    token = tokens[idx]
    if token.type == self.splittype:
        return [], 0
    token_string = self.getterfunc(token, doc=doc, feature=self.feature)
    if token_string is None:
        return [], 0
    if self.mapfunc:
        token_string = self.mapfunc(token_string)
    if self.ignorefunc:
        if self.ignorefunc(token_string):
            # no match possible here
            return [], 0
    # check if we can match the current token
    if token_string in self.nodes:
        # ok, we have the beginning of a possible match
        longest = 0
        node = self.nodes[token_string]
        thismatches = []
        thistokens = [token]
        if node.is_match:
            # the first token is already a complete match, so we need to add this to thismatches
            longest = 1
            # TODO: make this work with list data!
            if matchfunc:
                match = matchfunc(
                    idx, idx + 1, thistokens.copy(), node.data, node.listidx
                )
            else:
                match = TokenGazetteerMatch(
                    idx, idx + 1, thistokens.copy(), node.data, node.listidx
                )
            thismatches.append(match)
        j = idx + 1  # index into text tokens
        nignored = 0
        while j &lt; endidx:
            # print(f&#34;!!! processing idx={j}/{endidx}&#34;)
            if node.nodes:
                token = tokens[j]
                if token.type == self.splittype:
                    break
                token_string = self.getterfunc(token, doc=doc, feature=self.feature)
                if token_string is None:
                    j += 1
                    nignored += 1
                    continue
                if self.mapfunc:
                    token_string = self.mapfunc(token_string)
                if self.ignorefunc and self.ignorefunc(token_string):
                    j += 1
                    nignored += 1
                    continue
                if token_string in node.nodes:
                    node = node.nodes[token_string]
                    thistokens.append(token)
                    if node.is_match:
                        if matchfunc:
                            match = matchfunc(
                                idx,
                                idx + len(thistokens) + nignored,
                                thistokens.copy(),
                                node.data,
                                node.listidx,
                            )
                        else:
                            match = TokenGazetteerMatch(
                                idx,
                                idx + len(thistokens) + nignored,
                                thistokens.copy(),
                                node.data,
                                node.listidx,
                            )
                        # debugtxt = &#34; &#34;.join(
                        #     [doc[tokens[i]] for i in range(match.start, match.end)]
                        # )
                        # TODO: should LONGEST get calculated including ignored tokens or not?
                        if not longest_only:
                            thismatches.append(match)
                            if len(thistokens) &gt; longest:
                                longest = len(thistokens)
                        else:
                            if len(thistokens) &gt; longest:
                                thismatches = [match]
                                longest = len(thistokens)
                    j += 1
                    continue
                else:
                    break
            else:
                break
        return thismatches, longest
    else:
        # first token did not match, nothing to be found
        return [], 0</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="gatenlp.processing.gazetteer.base.GazetteerBase" href="base.html#gatenlp.processing.gazetteer.base.GazetteerBase">GazetteerBase</a></b></code>:
<ul class="hlist">
<li><code><a title="gatenlp.processing.gazetteer.base.GazetteerBase.__call__" href="../annotator.html#gatenlp.processing.annotator.Annotator.__call__">__call__</a></code></li>
<li><code><a title="gatenlp.processing.gazetteer.base.GazetteerBase.finish" href="../annotator.html#gatenlp.processing.annotator.Annotator.finish">finish</a></code></li>
<li><code><a title="gatenlp.processing.gazetteer.base.GazetteerBase.pipe" href="../annotator.html#gatenlp.processing.annotator.Annotator.pipe">pipe</a></code></li>
<li><code><a title="gatenlp.processing.gazetteer.base.GazetteerBase.reduce" href="../annotator.html#gatenlp.processing.annotator.Annotator.reduce">reduce</a></code></li>
<li><code><a title="gatenlp.processing.gazetteer.base.GazetteerBase.start" href="../annotator.html#gatenlp.processing.annotator.Annotator.start">start</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="gatenlp.processing.gazetteer.tokengazetteer.TokenGazetteerMatch"><code class="flex name class">
<span>class <span class="ident">TokenGazetteerMatch</span></span>
<span>(</span><span>*args, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="tokengazetteermatchstart-end-match-data-listidx">TokenGazetteerMatch(start, end, match, data, listidx)</h2>
<p>Create class TokenGazetteerMatch instance</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>recordclass.datatype.dataobject</li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="gatenlp.processing.gazetteer.tokengazetteer.TokenGazetteerMatch.data"><code class="name">var <span class="ident">data</span></code></dt>
<dd>
<div class="desc"><p>Return an attribute of instance, which is of type owner.</p></div>
</dd>
<dt id="gatenlp.processing.gazetteer.tokengazetteer.TokenGazetteerMatch.end"><code class="name">var <span class="ident">end</span></code></dt>
<dd>
<div class="desc"><p>Return an attribute of instance, which is of type owner.</p></div>
</dd>
<dt id="gatenlp.processing.gazetteer.tokengazetteer.TokenGazetteerMatch.listidx"><code class="name">var <span class="ident">listidx</span></code></dt>
<dd>
<div class="desc"><p>Return an attribute of instance, which is of type owner.</p></div>
</dd>
<dt id="gatenlp.processing.gazetteer.tokengazetteer.TokenGazetteerMatch.match"><code class="name">var <span class="ident">match</span></code></dt>
<dd>
<div class="desc"><p>Return an attribute of instance, which is of type owner.</p></div>
</dd>
<dt id="gatenlp.processing.gazetteer.tokengazetteer.TokenGazetteerMatch.start"><code class="name">var <span class="ident">start</span></code></dt>
<dd>
<div class="desc"><p>Return an attribute of instance, which is of type owner.</p></div>
</dd>
</dl>
</dd>
<dt id="gatenlp.processing.gazetteer.tokengazetteer.TokenGazetteerNode"><code class="flex name class">
<span>class <span class="ident">TokenGazetteerNode</span></span>
<span>(</span><span>is_match=None, data=None, nodes=None, listidx=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Represent an entry in the hash map of entry first tokens.
If is_match is True, that token is already a match and data contains the entry data.
The continuations attribute contains None or a list of multi token matches that
start with the first token and the entry data if we have a match (all tokens match).</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>is_match</code></strong></dt>
<dd>this node is a match</dd>
<dt><strong><code>data</code></strong></dt>
<dd>data associated with the match, can be a list of data items</dd>
</dl>
<p>nodes:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class TokenGazetteerNode:
    &#34;&#34;&#34;
    Represent an entry in the hash map of entry first tokens.
    If is_match is True, that token is already a match and data contains the entry data.
    The continuations attribute contains None or a list of multi token matches that
    start with the first token and the entry data if we have a match (all tokens match).
    &#34;&#34;&#34;

    __slots__ = (&#34;is_match&#34;, &#34;data&#34;, &#34;nodes&#34;, &#34;listidx&#34;)

    def __init__(self, is_match=None, data=None, nodes=None, listidx=None):
        &#34;&#34;&#34;

        Args:
            is_match: this node is a match
            data: data associated with the match, can be a list of data items
            nodes:
        &#34;&#34;&#34;
        self.is_match = is_match
        self.data = data
        self.listidx = listidx
        self.nodes = nodes

    @staticmethod
    def dict_repr(nodes):
        if nodes is not None:
            return str([(t, n) for t, n in nodes.items()])

    def __repr__(self):
        nodes = TokenGazetteerNode.dict_repr(self.nodes)
        return f&#34;Node(is_match={self.is_match},data={self.data},listidx={self.listidx},nodes={nodes})&#34;</code></pre>
</details>
<h3>Static methods</h3>
<dl>
<dt id="gatenlp.processing.gazetteer.tokengazetteer.TokenGazetteerNode.dict_repr"><code class="name flex">
<span>def <span class="ident">dict_repr</span></span>(<span>nodes)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def dict_repr(nodes):
    if nodes is not None:
        return str([(t, n) for t, n in nodes.items()])</code></pre>
</details>
</dd>
</dl>
<h3>Instance variables</h3>
<dl>
<dt id="gatenlp.processing.gazetteer.tokengazetteer.TokenGazetteerNode.data"><code class="name">var <span class="ident">data</span></code></dt>
<dd>
<div class="desc"><p>Return an attribute of instance, which is of type owner.</p></div>
</dd>
<dt id="gatenlp.processing.gazetteer.tokengazetteer.TokenGazetteerNode.is_match"><code class="name">var <span class="ident">is_match</span></code></dt>
<dd>
<div class="desc"><p>Return an attribute of instance, which is of type owner.</p></div>
</dd>
<dt id="gatenlp.processing.gazetteer.tokengazetteer.TokenGazetteerNode.listidx"><code class="name">var <span class="ident">listidx</span></code></dt>
<dd>
<div class="desc"><p>Return an attribute of instance, which is of type owner.</p></div>
</dd>
<dt id="gatenlp.processing.gazetteer.tokengazetteer.TokenGazetteerNode.nodes"><code class="name">var <span class="ident">nodes</span></code></dt>
<dd>
<div class="desc"><p>Return an attribute of instance, which is of type owner.</p></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="gatenlp.processing.gazetteer" href="index.html">gatenlp.processing.gazetteer</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="gatenlp.processing.gazetteer.tokengazetteer.tokentext_getter" href="#gatenlp.processing.gazetteer.tokengazetteer.tokentext_getter">tokentext_getter</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="gatenlp.processing.gazetteer.tokengazetteer.TokenGazetteer" href="#gatenlp.processing.gazetteer.tokengazetteer.TokenGazetteer">TokenGazetteer</a></code></h4>
<ul class="two-column">
<li><code><a title="gatenlp.processing.gazetteer.tokengazetteer.TokenGazetteer.add" href="#gatenlp.processing.gazetteer.tokengazetteer.TokenGazetteer.add">add</a></code></li>
<li><code><a title="gatenlp.processing.gazetteer.tokengazetteer.TokenGazetteer.append" href="#gatenlp.processing.gazetteer.tokengazetteer.TokenGazetteer.append">append</a></code></li>
<li><code><a title="gatenlp.processing.gazetteer.tokengazetteer.TokenGazetteer.find" href="#gatenlp.processing.gazetteer.tokengazetteer.TokenGazetteer.find">find</a></code></li>
<li><code><a title="gatenlp.processing.gazetteer.tokengazetteer.TokenGazetteer.find_all" href="#gatenlp.processing.gazetteer.tokengazetteer.TokenGazetteer.find_all">find_all</a></code></li>
<li><code><a title="gatenlp.processing.gazetteer.tokengazetteer.TokenGazetteer.get" href="#gatenlp.processing.gazetteer.tokengazetteer.TokenGazetteer.get">get</a></code></li>
<li><code><a title="gatenlp.processing.gazetteer.tokengazetteer.TokenGazetteer.match" href="#gatenlp.processing.gazetteer.tokengazetteer.TokenGazetteer.match">match</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="gatenlp.processing.gazetteer.tokengazetteer.TokenGazetteerMatch" href="#gatenlp.processing.gazetteer.tokengazetteer.TokenGazetteerMatch">TokenGazetteerMatch</a></code></h4>
<ul class="">
<li><code><a title="gatenlp.processing.gazetteer.tokengazetteer.TokenGazetteerMatch.data" href="#gatenlp.processing.gazetteer.tokengazetteer.TokenGazetteerMatch.data">data</a></code></li>
<li><code><a title="gatenlp.processing.gazetteer.tokengazetteer.TokenGazetteerMatch.end" href="#gatenlp.processing.gazetteer.tokengazetteer.TokenGazetteerMatch.end">end</a></code></li>
<li><code><a title="gatenlp.processing.gazetteer.tokengazetteer.TokenGazetteerMatch.listidx" href="#gatenlp.processing.gazetteer.tokengazetteer.TokenGazetteerMatch.listidx">listidx</a></code></li>
<li><code><a title="gatenlp.processing.gazetteer.tokengazetteer.TokenGazetteerMatch.match" href="#gatenlp.processing.gazetteer.tokengazetteer.TokenGazetteerMatch.match">match</a></code></li>
<li><code><a title="gatenlp.processing.gazetteer.tokengazetteer.TokenGazetteerMatch.start" href="#gatenlp.processing.gazetteer.tokengazetteer.TokenGazetteerMatch.start">start</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="gatenlp.processing.gazetteer.tokengazetteer.TokenGazetteerNode" href="#gatenlp.processing.gazetteer.tokengazetteer.TokenGazetteerNode">TokenGazetteerNode</a></code></h4>
<ul class="">
<li><code><a title="gatenlp.processing.gazetteer.tokengazetteer.TokenGazetteerNode.data" href="#gatenlp.processing.gazetteer.tokengazetteer.TokenGazetteerNode.data">data</a></code></li>
<li><code><a title="gatenlp.processing.gazetteer.tokengazetteer.TokenGazetteerNode.dict_repr" href="#gatenlp.processing.gazetteer.tokengazetteer.TokenGazetteerNode.dict_repr">dict_repr</a></code></li>
<li><code><a title="gatenlp.processing.gazetteer.tokengazetteer.TokenGazetteerNode.is_match" href="#gatenlp.processing.gazetteer.tokengazetteer.TokenGazetteerNode.is_match">is_match</a></code></li>
<li><code><a title="gatenlp.processing.gazetteer.tokengazetteer.TokenGazetteerNode.listidx" href="#gatenlp.processing.gazetteer.tokengazetteer.TokenGazetteerNode.listidx">listidx</a></code></li>
<li><code><a title="gatenlp.processing.gazetteer.tokengazetteer.TokenGazetteerNode.nodes" href="#gatenlp.processing.gazetteer.tokengazetteer.TokenGazetteerNode.nodes">nodes</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>